---
title: Part I, Data Simulation
editor_options: 
  chunk_output_type: console
html-math-method: 
  method: mathjax
---

## A Description of the Example and Aim of the Simulation

In this first task, we will deal with a domain that should be familiar to many developmental science researchers: infant looking times to adult-directed speech (ADS) and infant-directed speech (IDS). In this study, infant participants are exposed to recordings of ADS and IDS, and infants' looking times to an unrelated visual stimulus is recorded as the primary dependent variable. The key question is whether there are any behavioural differences according to the set of stimuli (i.e., ADS vs. IDS) within each participant. In this section, we will simulate data on the scale of looking times (i.e., 0-20,000ms) to gain fammiliarity with the simulation process. In later sections, we will extend the process to simulate data on the scale of effect sizes.

To give an overview of this first simulation task, we will simulate data with crossed random factors of subjects and stimuli and visualise the simulated data along the way to gain familiarity with the process. We will end up with a simulation function that can help us when performing the power analysis in Part II.

## Determining the Experimental Parameters

Before we can start to simulate data, we need to be very clear about the study design. This clarity is important because we need to define the parameters that govern the process we assume can give rise to the data. If we are building on ManyBabies1, then we are dealing with a within-subjects, between-items study; that is, each and every subject receives both ADS and IDS stimuli (within-subject), but each stimulus is either ADS or IDS (between-items).

Because infants are not the most patient of participants, perhaps a realistic study design would allow researchers to expose infants to 8 recordings of ADS and 8 recordings of IDS. And let's say that a realistic sample size our laboratory would be around 25 participants. This implies a total of 400 obsevations in this study (i.e., 8 + 8 recording stimuli for each of the 25 children). Let's set these experimental parameters now already

```{r}
# set number of subjects and items
n_subj     <- 25 # number of subjects
n_ADS  <-  8 # number of ADS stimuli
n_IDS <-  8 # number of IDS stimuli
```

## Data Generating Parameters

Now that we have an overview of the experimental design, we can start to consider a reasonable statistical model of the data. In the following sections, we will build up the parameters for a mixed-effects statistical model of the following type, as described in the lecture:

$$
Looking Time = \beta_0 + \beta_1 \cdot \text{SpeechStyle} + u_{0j}^{(item)} + u_{0i}^{(subj)} + (u_{1i}^{(subj)} \cdot \text{SpeechStyle}) + \varepsilon
$$

According to this formula, the looking time for subject, s, on item, i, is decomposed into linear combination of fixed effects (i.e., the population grand mean, Œ≤‚ÇÄ, and the effect of SpeechStyle, Œ≤‚ÇÅ) and a variety of random effects, such as a by-subject random intercept, u‚ÇÄ·µ¢^{(subj)}, a by-item random intercept u‚ÇÄ‚±º^{(item)}, a by-subject random slope, (u‚ÇÅ·µ¢^{(subj)} * SpeechStyle), and a trial-level residual error, Œµ.

This formula paves a clear way forward for our data simulation. In the next section, we will build up a statistical model step by step, defining variables in the code as we go along that reflect our choices for parameters.

## Choosing Values for Fixed Effect Parameters

```{r setup, include=FALSE}
#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages
options(repos = "https://cran.r-project.org/")
install.packages('pacman')
#load the required packages:
pacman::p_load(knitr, # rendering of .Rmd file
               lme4, # model specification / estimation
               afex, # anova and deriving p-values from lmer
               broom.mixed, # extracting data from model fits
               faux, # generate correlated values
               tidyverse, # data wrangling and visualisation
               ggridges, #visualisation
               viridis, # color schemes for visualisation
               kableExtra, #helps with knitting the .Rmd file
               cowplot, #visualisation tool to include multiple plots
               ggrain, #visualisation tool for raincloud plots
               dplyr,
               here,
               tidyr
               )

set.seed(1234)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)

plot_theme <- 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 18), 
        axis.text.x = element_text(size = 13), 
        axis.title.x = element_text(size = 13), 
        axis.text.y = element_text(size = 12), 
        axis.title.y = element_text(size = 13),
        strip.background = element_rect(color="white", fill="white", size=1.5, linetype="solid"))

```

Let's start by setting the fixed-effect parameter of SpeechStyle (Œ≤‚ÇÄ + Œ≤‚ÇÅ*SpeechStyle). How should we set these parameters? First of all, we can be guided by what we know about looking time distributions in infant experiments. For example, we could imagine average infant looking times to be around 7 seconds (i.e., 7000ms), and from ManyBabies1, we know that infants exhibit around X seconds (i.e., 2000ms) longer looking times to IDS stimuli than to ADS stimuli. Let's go ahead and code these values explicitly as parameter values in our simulation.

```{r}
# set fixed effect parameters
mean_intercept <- 7000 # intercept; i.e., the grand mean, Œ≤‚ÇÄ
mean_slope <- 2000 # slope; i.e, effect of IDS, Œ≤‚ÇÅ
```

When we're modelling data from experiments involving individuals (like infants in this case), it's essential to account for the fact that each individual may have a unique baseline reaction to the stimuli, which we can't anticipate or control for entirely. Similarly, the effect of the stimuli itself might vary across different instances of the stimulus. In other words, we expect variability both in how subjects react and in the impact of each stimulus. To address this, we introduce random intercept values for subjects and items. This means we allow for each subject and each stimulus item to have its own intercept value, capturing their individual baseline reactions and effects. To quantify the variability around these values, we introduce the standard deviation of the random intercepts, reflecting the range of differences we might observe among subjects' reactions and the effects of different stimulus items. Let's code this as standard deviation of by-subject random intercept and by-item random intercept sd.

## Choosing Values for Varying Slope Parameters
```{r}
# set random effect parameters
subject_varyingintercept   <- 2000 # by-subject random intercept sd
item_varyingintercept <- 1000   # by-item random intercept sd
```

In modeling the effect of IDS stimuli on infant looking times, it's also important to acknowledge that this effect may vary across different infants. Some infants might be more responsive to IDS stimuli than others, leading to differences in their looking times. Therefore, we need to introduce subject-specific variability to account for how the effect of IDS varies among infants. In statistical modeling, when we include random slopes, we're essentially allowing the effect of certain variables to vary not only across different individuals (random intercepts) but also across different levels of another variable (random slopes). 

When we introduce random slopes, we need to consider potential correlations between these varying slopes and the varying intercepts. This is because if there's a correlation between the way individuals (infants, in this case) respond to the IDS stimuli (reflected in the random slopes) and their baseline behaviors (reflected in the random intercepts), it can affect our model's predictions. For instance, if infants who naturally have longer attention spans (reflected in higher random intercepts) also tend to show stronger responses to IDS stimuli (reflected in steeper random slopes), ignoring this correlation might lead to biased estimates or inaccurate predictions. Including a correlation matrix allows us to explicitly account for these potential correlations, ensuring that our model accurately captures the relationships between different sources of variability in the data and produces more reliable results. To capture these patterns, we include a correlation matrix, specifying a weak correlation between the varying intercepts and varying slopes of infant participants. Lastly, we incorporate a residual error term to account for any unexplained sources of variability in the model. This helps ensure that our model accurately captures the complexities of infants' responses to different stimuli while also acknowledging inherent variations and correlations within the data.

```{r}
# set more random effect and error parameters
subject_varyingslope  <-  1000 # by-subject random slope sd
rho    <-  0.2 # correlation between intercept and slope
sigma  <- 500 # residual (error) sd
```

## Simulate the sampling of stimulus items

Now it's time to create a dataset that lists, for each stimulus item, the speech style it is in and its varying properties on infants' looking times. This captures the intuition that we would expect different speech recordings to exhibit variation in infants' average looking times. Here, we model items only with varying intercepts. To set the parameter for varying item intercept, we are going to specify the standard deviation that we expect items to exhibit in the parameter, item_varyingintercept, in the below code. That is, we sample values from a normal distribution using the rnorm() function, with a meaan of 0 and standard deviation of item_varyingintercept. For the varying item variable, we also need to assign a unique identifer to each of the 16 speech stimuli and designate whether the stimuli are ADS or IDS, with the first 8 being ADS and the next 8 being IDS. We are going to use the faux package to carry this out. Lastly, we will carry out contrast coding, and we will later multiply this effect-coded factor by the fixed effect of category to simulate data where the ADS stimuli on average generate looking times of -500 ms different from the grand mean, while the IDS stimuli are on average 1000 ms different from the grand mean.

```{r}
# simulate a sample of items
# total number of items = n_ADS + n_IDS
items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_varyingintercept)
) %>% 
  mutate(item_id = faux::make_id(nrow(.), "I")) %>%
  mutate(SpeechStyle = recode(Register, "ADS" = -0.5, "IDS" = +0.5)) #introduce a numeric predictor to represent what category each stimulus item, we set ADS to be -0.5 and IDS to be +0.5. This is what is known as contrast coding.
```

To get to better grips with the simuation process, let's the data visualise and take a look at what we have produced:

```{r}
glimpse(items)

ggplot(items, aes(1, O_0i, fill = Register, color = Register)) +
  geom_rain(alpha = .8,
            boxplot.args = list(color = "black", outlier.shape = NA)) +
  ggtitle('Varying Intercept Terms for Stimulus Item') +
  ylab('SD of Item Intercept (ms)') +
  facet_wrap(~Register) +
  scale_fill_brewer(palette = 'Dark2') +
  scale_color_brewer(palette = 'Dark2') +
  plot_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

## Simulate the sampling of subjects

Now, we will simulate the sampling of individual subjects to create a table that lists each subject along with their two correlated varying effects, namely intercepts and slopes. This process is slightly more complex than before because we cannot simply sample the intercept values (ùëá0ùë†) independently from the slope values (ùëá1ùë†) using rnorm(). Instead, we need to sample pairs of ‚ü®ùëá0ùë†,ùëá1ùë†‚ü© values for each subject from a bivariate normal distribution. We will use the rnorm_multi() function from the faux package (DeBruine 2020) to carry this out. This function generates a table of simulated values from a multivariate normal distribution, allowing us to specify the means (mu) and standard deviations (sd) for each variable, along with the correlations (r), which can be a single value applied to all pairs, a correlation matrix, or a vector of values representing the upper right triangle of the correlation matrix.

```{r}
# simulate a sample of subjects
# sample from a multivariate random distribution 
subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(subject_varyingintercept, subject_varyingslope), # note that we set the SDs further up in the code when specifying varying intercepts and sloeps.
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S")) # add subject ids that correspond to the number of rows simulated.
```

Again, let's visualise this process, so that we are sure what is going on:

```{r}
glimpse(subjects)

subjects %>%
  pivot_longer(cols = starts_with("T"),
               names_to = "T_variable",
               values_to = "value") %>%
  ggplot() + 
  geom_density(aes(value, fill = T_variable), alpha = 0.8) +
  xlim(c(-4*subject_varyingintercept, 4*subject_varyingintercept)) +
  facet_wrap(~T_variable) +
  ggtitle('Varying Intercept and Slope Terms for Subjects') +
  plot_theme +
  scale_fill_brewer(palette = 'Dark2') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

## Time to Put It All Together

Because all subjects respond to all items, we can set up a table of trials by making a table with every possible combination of the rows in the subject and item tables using the tidyverse function crossing(). This function generates every possible combination of rows from the subject and item tables, ensuring that each subject's responses to each item are included in the dataset. To introduce variability reflecting the inherent fluctuations in trial-by-trial performance, we incorporate random error into each trial. This randomness is simulated by sampling values from a normal distribution with a mean of 0 and a standard deviation of sigma. This approach ensures that our dataset captures the full range of subject-item interactions while accounting for unpredictable variations in individual performance across trials.

```{r}
# cross subject and item IDs; add an error term
# nrow(.) is the number of rows in the table
ParameterValues <- crossing(subjects, items)  %>%
  mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma))

glimpse(ParameterValues)
```

Now we have specified the parameters in ParameterValues, we are ready to add up everything together to create the response variable (i.e., infant looking times in milliseconds). To be more specific, we calculate the response variable, looking time, by adding together:

* the grand intercept (mean_intercept),
* each subject-specific random intercept (T_0s),
* each item-specific random intercept (O_0i),
* each sum of the speech style effect (mean_slope) and the random slope (T_1s), multiplied by the numeric predictor (SpeechStyle), and
* each residual error (e_si).

```{r}
# calculate the response variable
SimulatedLT <- ParameterValues %>%
  mutate(LT = mean_intercept + T_0s + O_0i + (mean_slope + T_1s) * SpeechStyle + e_si) %>% #sum together overall intercept, varying subject and item intercepts, varying subject slopes, and random error.
  mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %>% #add a long tail to the distribution to simulate exgaussian distribution of looking times
  dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)
```

Let's have a look at what the data we have generated looks like:

```{r, warning = FALSE}
# Plot of how overall looking time distributions differ across ADS and IDS
SimulatedLT %>%
  ggplot() + 
  geom_density(aes(LT, fill = Register), alpha = 0.8) +
  ggtitle('Varying Intercept and Slope Terms for Subjects') +
  plot_theme +
  scale_fill_brewer(palette = 'Dark2') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

# Plot of how looking times of indvidual subjects differ across the two speech style
SimulatedLT %>%
  group_by(subj_id, Register) %>%
  dplyr::summarise(medLT = mean(LT), .groups = 'drop') %>%
  ggplot(aes(x = Register, y = medLT, fill = Register)) + 
  geom_rain(alpha = 0.8, rain.side = "f1x1", id.long.var = "subj_id", point.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42))) + 
  scale_fill_brewer(palette = 'Dark2') +
  ggtitle('Individual Subject Looking Times across Speech Styles') + 
  xlab("Speech Style") + 
  ylab('Looking Time (ms)') + 
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```

## With the Initial Setup Done, Let's Automatise!
Now that we've simulated a dataset with the necessary properties, suitable for sophisticated linear mixed effects models, we can streamline the process by encapsulating all the preceding code into a custom function. This function will accept the parameters we defined earlier as arguments, with default values set to our chosen parameters. By doing so, we are empowered to effortlessly experiment with different parameters or generate multiple datasets for power analysis purposes. The following code condenses all the preceding steps into a single function, returning a dataset with the specified parameters.

```{r}
# set up the custom data simulation function
SimulateLTData <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  mean_intercept = 7000,   # grand mean
  mean_slope =  2000,   # effect of category
  item_varyingintercept =  200,   # by-item random intercept sd
  subject_varyingintercept = 2000,   # by-subject random intercept sd
  subject_varyingslope =  1000,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 500) { # residual (standard deviation)

  items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_varyingintercept)) %>% 
  mutate(item_id = faux::make_id(nrow(.), "I")) %>%
  mutate(SpeechStyle = recode(Register, "ADS" = -0.5, "IDS" = +0.5))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(subject_varyingintercept, subject_varyingslope), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S"))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())
  
  ParameterValues %>%
    mutate(LT = mean_intercept + T_0s + O_0i + (mean_slope + T_1s) * SpeechStyle + e_si) %>%
    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)
}
```


## Exercises to Check Understanding

### Exercise I

* How would you adapt the above code to generate a dataset with 500 participants and no effect of SpeechStyle (i.e., distributions similar to the below plot)?

```{r, warning = FALSE}
#| code-fold: true
#| code-summary: "Show the code"

#With our new SimulateLTData() function, the answer here is fairly straightforward! We can simply specify that we want to simulate 500 subjects and want a mean slope of 0, like so: SimulateLTData(n_subj = 500, mean_slope = 0).

LTDataSimulated <- SimulateLTData(n_subj = 500, mean_slope = 0)

LTDataSimulated %>%
  group_by(subj_id, Register) %>%
  dplyr::summarise(medLT = mean(LT), .groups = 'drop') %>%
  ggplot(aes(x = Register, y = medLT, fill = Register)) + 
  geom_rain(alpha = 0.8, rain.side = "f1x1", id.long.var = "subj_id", point.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42))) + 
  scale_fill_brewer(palette = 'Dark2') +
  ggtitle('Looking Time Differences across Speech Styles') + 
  xlab("Speech Style") + 
  ylab('Looking Time (ms)') + 
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```

### Exercise II

* We might expect the IDS preference effect to change with infant age, such that older infants display longer looking times to IDS over ADS. How would you add a positive interaction effect of (cross-sectional) age as a predictor to the model (hint: it involves randomly sampling age for each child and adding an effect to the simulation code and model)? Try to think through the problem, get inspiration from the below plot, code up a solution, and only then click on "Show the code" to check how you might approach this.

```{r, warning = FALSE}
#| code-fold: true
#| code-summary: "Show the code"

#The question here involves adding infant age as an interaction effect with SpeechStyle. We will approach this question by modifying the code that simulates subject-level data. Here, we will sample an age variable and pretend that we have standardised age so that its values fall between -0.5 and 0.5. We thus randomly sample and age to assign one age to each subject. We also need to specify a slope value for the influence of subject ag and place it in the start of the function; we will add subject_age = 200. Lastly, we need to change how we sum the values together, so that age has an effect on looking times, but also that the influence of age exerts different effects across the two speech styles.

# set up the custom data simulation function
SimulateLTDataWithAge <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  beta_0 = 7000,   # grand mean
  beta_1 =  2000,   # effect of category
  beta_as = 5000,
  S_as = 200,
  item_sd =  200,   # by-item random intercept sd
  tau_0 = 2000,   # by-subject random intercept sd
  tau_1 =  1000,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 500) { # residual (standard deviation)

  items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_sd)) %>% 
  mutate(item_id = faux::make_id(nrow(.), "I")) %>%
  mutate(SpeechStyle = recode(Register, "ADS" = 0, "IDS" = 1))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1, S_as), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s", "S_as")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S")) %>%
  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())
  
  ParameterValues %>%
    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + e_si) %>%
    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, LT)
}

DataWithAgeSimulated <- SimulateLTDataWithAge()
DataWithAgeSimulated %>%
ggplot() + 
  geom_point(aes(y = LT, x = age_subj, color = subj_id), alpha = 0.6, size = 1, show.legend = F) + 
  geom_smooth(method = "lm", se = TRUE, formula = y ~ x, aes(y = LT, x = age_subj)) +
  ggtitle("Age as Positive Interaction Effect") +
  xlab("Age (standardised age)") + 
  facet_wrap(~Register) +
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```

### Exercise III

* As mentioned above, we made the simplifying assumption that each and every stimulus item evokes exactly the same response in participants; however, certain items might elicit stronger or weaker responses depending on individual differences or contextual factors, so including varying slopes for stimulus items can help capture this heterogeneity more accurately. How could we modify the above code to include varying slopes according stimulus items? Again, try to think through the problem, get inspiration from the below plot, code up a solution, and only then click on "Show the code" to check how you might approach this.

```{r, warning = FALSE}
#| code-fold: true
#| code-summary: "Show the code"

# set up the custom data simulation function
SimulateLTDataWithAge <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  beta_0 = 7000,   # grand mean
  beta_1 =  2000,   # effect of category
  beta_as = 5000,
  S_as = 200,
  item_sd =  200,   # by-item random intercept sd
  item_slope_sd =  200,   # by-item random slope sd
  tau_0 = 2000,   # by-subject random intercept sd
  tau_1 =  1000,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 500) { # residual (standard deviation)

  items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  faux::rnorm_multi(
  n = n_ADS + n_IDS, 
  mu = 0, # means for random effects are always 0
  sd = c(item_sd, item_slope_sd),
  r = rho,
  varnames = c("item_sd", "item_slope_sd"))) %>%
    mutate(item_id = faux::make_id(n_ADS + n_IDS, "I")) %>%
    mutate(SpeechStyle = recode(Register, "ADS" = 0, "IDS" = 1))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1, S_as), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s", "S_as")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S")) %>%
  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())
  
  ParameterValues %>%
    mutate(LT = beta_0 + T_0s + item_sd + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + (beta_1 + item_slope_sd) * SpeechStyle + e_si) %>%
    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, LT)
}

DataWithAgeSimulated <- SimulateLTDataWithAge()
DataWithAgeSimulated %>%
ggplot() + 
  geom_point(aes(y = LT, x = age_subj, color = subj_id), alpha = 0.6, size = 1, show.legend = F) + 
  geom_smooth(method = "lm", se = TRUE, formula = y ~ x, aes(y = LT, x = age_subj)) +
  ggtitle("Age as Positive Interaction Effect") +
  xlab("Age (standardised age)") + 
  facet_wrap(~Register) +
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```
