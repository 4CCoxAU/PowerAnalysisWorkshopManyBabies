---
title: Part II, Modelling the Simulated Data
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages
options(repos = "https://cran.r-project.org/")
install.packages('pacman')
#load the required packages:
pacman::p_load(knitr, # rendering of .Rmd file
               lme4, # model specification / estimation
               afex, # anova and deriving p-values from lmer
               broom.mixed, # extracting data from model fits
               faux, # generate correlated values
               tidyverse, # data wrangling and visualisation
               ggridges, #visualisation
               viridis, # color schemes for visualisation
               kableExtra, #helps with knitting the .Rmd file
               cowplot, #visualisation tool to include multiple plots
               ggrain, #visualisation tool for raincloud plots
               dplyr,
               here,
               tidyr,
               readr
               )

set.seed(1234)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)

plot_theme <- 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 18), 
        axis.text.x = element_text(size = 13), 
        axis.title.x = element_text(size = 13), 
        axis.text.y = element_text(size = 12), 
        axis.title.y = element_text(size = 13),
        strip.background = element_rect(color="white", fill="white", size=1.5, linetype="solid"))

```

## Cumulative Science and Prior Knowledge

We know that science stands on the shoulder of – not giants – but normal human beings who are as susceptible to confirmation and selection biases as everyone else. And up until now, we have relied on our intuitions about infant looking times based solely on our experience with conducting infant experiments. In the first exercise, it quickly became clear that we relied heavily on our own intuitions about what we know about the domain of infant looking times. In this section, we consider how to improve our data simulation process by capitalising on cumulative science efforts, such as meta-analyses and multi-lab replication studies. Seeing scientific advancement as an iterative procedure involving data accumulation and synthesis really empowers us to map out the diversity of samples in earlier research, scrutinise the possibilities for generalisability, and point to future directions of research.

For the IDS preference effect, for example, we can thank the ManyBabies community for conducting both a multi-lab replication study and a community-augmented meta-analysis on infants’ preference to attend to IDS over ADS (ManyBabies Consortium, 2020; Zettersten, Cox, et al., 2023). By synthesising data across such a wide variety of experimental designs, participants and stimuli, we now have a fairly good estimate of the overall magnitude of the IDS preference effect. Both sources of evidence converge on an effect size of 0.35 with 95% CI of [0.16; 0.47].

When calculating power, we need to leverage results from these cumulative science efforts so that we can engage in iterative development of our estimates and theories. This section delves into the realm of effect sizes and teaches you how to implement a power analysis based on an effect size estimate. 


## Simulating Effect Size Data and Making Informed Choices

Let's continue with our IDS preference example and take inspiration from the ManyBabies1 estimate (https://doi.org/10.1177/2515245919900809) to think about how we would simulate data for a new experimental study on the IDS preference effect. Because we are still interested in a within-subjects, between-items study, we can rely on the simulation function from the previous page. All we have to do is to adapt the simulation function so that it suits the new scale of effect sizes. Effect size is a simple way of quantifying the size of the difference between two groups. It allows us to move beyond the “Does it work?” question to “How well does it work in a range of contexts?”. In short, the effect size is based on the standardised mean difference, and quantifies the difference between two means in terms of standard deviation units; an effect size of 0.35 thus implies a difference between the two groups of 0.35 standard deviations.

## Adapting our Simulation Function
Let's adapt our simulation function from previous pages to the new scale of effect sizes and call it SimulateEffectSizeData(). Following ManyBabies Consortium (2020), a positive effect size denotes longer looking times to IDS stimuli over ADS stimuli, and an effect size of 0 denotes no preference for either speech style (i.e., similar looking times to ADS and IDS stimuli).

```{r}
# set up the custom data simulation function
SimulateEffectSizeData <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  mean_intercept = 0,   # grand mean
  mean_slope =  0.35,   # effect of category
  item_varyingintercept =  0.05,   # by-item random intercept sd
  item_varyingslope =  0.05,   # by-item random intercept sd
  subject_varyingintercept = 0.1,   # by-subject random intercept sd
  subject_varyingslope =  0.1,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 0.3) { # residual (standard deviation)

    items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  faux::rnorm_multi(
  n = n_ADS + n_IDS, 
  mu = 0, # means for random effects are always 0
  sd = c(item_varyingintercept, item_varyingslope),
  r = rho,
  varnames = c("item_sd", "item_slope_sd"))) %>%
    mutate(item_id = faux::make_id(n_ADS + n_IDS, "I")) %>%
    mutate(SpeechStyle = recode(Register, "ADS" = 0, "IDS" = 1))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(subject_varyingintercept, subject_varyingslope), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("subject_sd", "subject_slope_sd")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S"))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())
  
  ParameterValues %>%
    mutate(EF = mean_intercept + subject_sd + item_sd + (mean_slope + subject_slope_sd + item_varyingslope) * SpeechStyle + e_si) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, EF)
}
```

```{r}
EffectSizeDataSimulated <- SimulateEffectSizeData()
EffectSizeDataSimulated %>%
    ggplot() + 
  geom_density(aes(EF, fill = Register), alpha = 0.8) +
  geom_vline(xintercept = 0.35, linetype = 3) +
  geom_vline(xintercept = 0, linetype = 3) +
  xlim(c(-5*0.35, 5*0.35)) +
  ggtitle("IDS Preference Effect in Effect Sizes") +
  plot_theme + 
  scale_fill_brewer(palette = "Dark2") + 
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank())
```

```{r, warning = FALSE}
EffectSizeDataSimulated %>%
  group_by(subj_id, Register) %>%
  dplyr::summarise(medLT = mean(EF), .groups = 'drop') %>%
ggplot(aes(x = Register, y = medLT, fill = Register)) + 
  geom_rain(alpha = 0.8, rain.side = "f1x1", id.long.var = "subj_id", point.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42))) + 
  scale_fill_brewer(palette = 'Dark2') +
  ggtitle('Subject-Level Differences across Speech Styles') + 
  xlab("Speech Style") + 
  ylab('Effect Size') + 
  plot_theme
```

## A Linear Mixed Effects Model of Simulated Effect Size Data

Let's think about how we want to run a linear mixed-effects model of the data. For a varying-intercepts, varying-slopes model for each subject, we could run the following model with the lmer syntax as follows:

* EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)

In this model, EF is the effect size response variable; 1 corresponds to the average intercept; SpeechStyle is the predictor for the ADS/IDS manipulation for item i; (1 | item_id) specifies a by-subject random intercept (O_0i); (1 + SpeechStyle | subj_id) specifies a subject-specific random intercept (T_0s) plus the subject-specific random slope of SpeechStyle (T_1s). The error term (e_si) is automatically included in all models, so is left implicit in the above formula.  The terms in parentheses with the “pipe” separator (|) define the random effects structure. For each of these bracketed terms, the left-hand side of the pipe names the effects you wish to allow to vary and the right hand side names the variable identifying the levels of the random factor over which the terms vary (e.g., subjects or items). The first term, (1 | item_id) allows the intercept (1) to vary over the random factor of items (item_id). This is an instruction to estimate the parameter underlying the O_0i values, namely omega_0. The second term, (1 + X_i | subj_id), allows both the intercept and the effect of category (coded by X_i) to vary over the random factor of subjects (subj_id). It is an instruction to estimate the three parameters that underlie the T_0s and T_1s values, namely tau_0, tau_1, and rho.

```{r}
dataSimulated <- SimulateEffectSizeData()

model <- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id), 
               data = dataSimulated)

summary(model)
```

Try to run this a couple of times with new simulated data and see how the estimates and significance measures change. In the following code blocks, we will automatise this process and run the power analysis proper!

## Time to Power-Up the Analysis

Now we have two essential components to perform a simulation-based power analysis: i) a code pipeline to generate data for our research question and ii) a clear idea of how we want to model the data. Now it's time to run the actual power analysis. The way we do this is to specify an effect, run 100s of models and count how many of the models show significant effects. To accomplish this, we can write a new function that combines a modelling component into our previous SimulateEffectSizeData() function. That is, if we include both the simulation and modelling of the data in one function, we can simplify the process of performing a power analysis. We will call this function SimulateAndModelEFData(), and we will use broom.mixed::tidy(model) to obtain a dataframe with relevant results from the model and write them to a .csv-file. 

```{r, warning = FALSE}
# simulate, analyze, and return a table of parameter estimates
SimulateAndModelEFData <- function(...) {
  #simulate EF data function
  dataSimulated <- SimulateEffectSizeData()
  
  #model EF data
  model <- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id),
                data = dataSimulated)
  #write to a dataframee
  broom.mixed::tidy(model)
}
```

## Running the Power Analysis

Now we have a function to generate data, run our model and spit out the results! Now it's time to repeat a few hundred times, so that we can calculate how much power we have with our given parameters. We are going to use map_df() to run the simulation and modelling function 500 times and write it to a .csv file.

```{r, echo=FALSE, message=FALSE, results='hide',warning = FALSE}
# run simulations and save to a file
n_runs <- 500 # use at least 500 to get stable estimates
simulations <- purrr::map_df(1:n_runs, ~ SimulateAndModelEFData())
write_csv(simulations, "EFsimulations.csv")
```

If it ran correctly, it should have produced a .csv file with model results from each new simulation of data. Let's read in the results and have a look at what they say!

```{r, warning = FALSE}
# read saved simulation data
sims <- read_csv("EFsimulations.csv", show_col_types = FALSE) %>%
  dplyr::select(term, estimate, std.error, p.value)

sims %>% 
  group_by(term) %>%
  dplyr::summarize(
    mean_estimate = mean(estimate),
    mean_se = mean(std.error),
    power = mean(p.value < 0.05),
    .groups = "drop"
  )
```

## Exercises to Check Understanding

### Exercise IV

Now that we have this pipeline set up, it becomes easy to adapt the code to try out different parameter values. Let's explore the effect of repeated measures on power. Try to run a power analysis with each subject receiving two items in each speech style. What happens to the estimate of statistical power?

```{r, echo=FALSE, message=FALSE, results='hide', warning = FALSE}
#| code-fold: true
#| code-summary: "Show the code"
 
# simulate, analyze, and return a table of parameter estimates
SimulateAndModelEFData <- function(...) {
  #simulate EF data function
  dataSimulated <- SimulateEffectSizeData(n_ADS = 2, n_IDS = 2)
  
  #model EF data
  model <- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id),
                data = dataSimulated)
  #write to a dataframee
  broom.mixed::tidy(model)
}
# run simulations and save to a file
n_runs <- 500 # use at least 500 to get stable estimates
simulations <- purrr::map_df(1:n_runs, ~ SimulateAndModelEFData())
write_csv(simulations, "EFsimulations2Stimuli.csv")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# read saved simulation data
sims <- read_csv("EFsimulations2Stimuli.csv", show_col_types = FALSE) %>%
  dplyr::select(term, estimate, std.error, p.value)

sims %>% 
  group_by(term) %>%
  dplyr::summarize(
    mean_estimate = mean(estimate),
    mean_se = mean(std.error),
    power = mean(p.value < 0.05),
    .groups = "drop"
  )
```

### Exercise V

Let's imagine a scenario where we are interested in the effect of age on IDS preference. We would like to explore the extent to which we can detect a cross-sectional age effect given only two stimulus items per participant. How would adapt the above code to explore this experimental design? 

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| 
# set up the custom data simulation function
SimulateEFDataWithAge <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  beta_0 = 0,   # grand mean
  beta_1 =  0.1,   # effect of category
  beta_as = 0.4,
  S_as = 0.2,
  item_sd =  0.2,   # by-item random intercept sd
  tau_0 = 0.1,   # by-subject random intercept sd
  tau_1 =  0.1,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 0.3) { # residual (standard deviation)

  items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_sd)) %>% 
  mutate(item_id = faux::make_id(nrow(.), "I")) %>%
  mutate(SpeechStyle = recode(Register, "ADS" = 0, "IDS" = 1))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1, S_as), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s", "S_as")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S")) %>%
  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())
  
  ParameterValues %>%
    mutate(EF = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + e_si) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, EF)
}

EFDataWithAgeSimulated <- SimulateEFDataWithAge()
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| 
EFDataWithAgeSimulated <- SimulateEFDataWithAge()
EFDataWithAgeSimulated %>%
ggplot() + 
  geom_point(aes(y = EF, x = age_subj, color = subj_id), alpha = 0.6, size = 1, show.legend = F) + 
  geom_smooth(method = "lm", se = TRUE, formula = y ~ x, aes(y = EF, x = age_subj)) +
  ggtitle("Age as Positive Interaction Effect") +
  xlab("Age (standardised age)") + 
  facet_wrap(~Register) +
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```

```{r, warning = FALSE, echo=FALSE, message=FALSE, results='hide', warning = FALSE}
#| code-fold: true
#| code-summary: "Show the code"
 
# simulate, analyze, and return a table of parameter estimates
SimulateAndModelEFAgeData <- function(...) {
  #simulate EF data function
  dataSimulated <- SimulateEFDataWithAge(n_ADS = 4, n_IDS = 4)
  
  #model EF data
  model <- lmer(EF ~ 1 + SpeechStyle:age_subj + (1 | item_id) + (1 + SpeechStyle | subj_id),
                data = dataSimulated)
  #write to a dataframee
  broom.mixed::tidy(model)
}

# run simulations and save to a file
n_runs <- 500 # use at least 500 to get stable estimates
simulations <- purrr::map_df(1:n_runs, ~ SimulateAndModelEFAgeData())
write_csv(simulations, "EFsimulationsAge.csv")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# read saved simulation data
sims <- read_csv("EFsimulationsAge.csv", show_col_types = FALSE) %>%
  dplyr::select(term, estimate, std.error, p.value)

sims %>% 
  group_by(term) %>%
  dplyr::summarize(
    mean_estimate = mean(estimate),
    mean_se = mean(std.error),
    power = mean(p.value < 0.05),
    .groups = "drop"
  )
```

Now we have a pretty great pipeline set up that allows us to explore the effects of different parameter values on our ability to detect effects. However, instead of manually varying the parameters one by one, it would be nice if we could set up a grid search to explore values and put the power results into perspective. We will explore how to do this in the next exercise sheet. The code gets slightly more complepx, so make sure that you have understood the code that we have written so far before venturing further. 
