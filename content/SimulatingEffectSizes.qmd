---
title: Modelling Simulated Effect Size Data
editor_options: 
  chunk_output_type: console
---

Let's start by loading the required packages:

```{r setup, include=TRUE}
#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages
#install.packages('pacman')
#load the required packages:
pacman::p_load(knitr, # rendering of .Rmd file
               lme4, # model specification / estimation
               afex, # anova and deriving p-values from lmer
               broom.mixed, # extracting data from model fits
               faux, # generate correlated values
               tidyverse, # data wrangling and visualisation
               ggridges, #visualisation
               viridis, # color schemes for visualisation
               kableExtra, #helps with knitting the .Rmd file
               cowplot, #visualisation tool to include multiple plots
               ggrain,
               dplyr,
               here,
               tidyr)
set.seed(1234)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)

plot_theme <- 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 18), 
        legend.position = "none", 
        axis.text.x = element_text(size = 13), 
        axis.title.x = element_text(size = 13), 
        axis.text.y = element_text(size = 12), 
        axis.title.y = element_text(size = 13))
```

## Cumulative Science Perspective

Science stands on the shoulders of – not giants – but normal human beings who are as susceptible
to confirmation and selection biases as everyone else. The limitations of relying on single studies have motivated a push towards methods of
synthesising quantitative evidence in the form of meta- and mega-analyses (Koile & Cristia, 2021;
Liu & Almeida, 2023; Zettersten et al., 2023). To counter selection biases and obtain an objective
picture of a given field, there exist detailed procedures restricting researcher degrees of freedom
in the process (Moher et al., 2009; Page et al., 2021). Synthesising evidence from studies in a
cumulative approach offers insights to the generalisability and heterogeneity of the construct
across a wide variety of experimental designs, participants and stimuli (Cristia et al., 2022; Cruz
Blandón et al., 2023; Koile & Cristia, 2021; Tsuji et al., 2014). There are limitations to this process
of estimation across studies, such as issues in transparency and reproducibility (Lakens et al., 2016,
2017; Nuijten et al., 2020), considerable errors of extraction (Zettersten, Cox, et al., 2023),
reliance on published literature and concomitant bias (Moher et al., 2009; Page et al., 2021), as
well as substantial between-study heterogeneity threatening practical interpretation of results
(Mathur & VanderWeele, 2019). There are statistical approaches that attempt to correct for
publication bias and to account for heterogeneity; however, major concerns about the validity of
meta-analytic estimates remain (cf., Kvarven et al., 2020). These concerns primarily revolve
around the reliability of meta-analytic estimates rather than the synthesis of insights and
exploration of the sampling space of individual studies (Mathur & VanderWeele, 2020; Zettersten,
Cox, et al., 2023). A related cumulative method that synthesises findings across multiple individual
experiments involves multi-lab replication studies. In such designs, multiple labs conduct
replications of original studies by implementing a common experimental protocol to test a research question across sites (ManyBabies Consortium, 2020). By varying experimenter identity and
increasing sample diversity, these large-scale studies contribute to a greater likelihood of
generalisability and precision (Byers-Heinlein et al., 2021; ManyBabies Consortium, 2020).
However, the high degree of uniformity in methodological and analytic implementation can lead
to less generalisability across other conditions than a meta-analysis.

In a recent study (Zettersten, Cox, et al., 2023), we attempted to reconcile the results from
a multi-lab replication study (ManyBabies Consortium, 2020) and a community-augmented metaanalysis
on infants’ preference to attend to IDS over ADS. The findings from each source
individually provided an incomplete picture of moderators of the IDS preference effect; for
example, the two studies differed with respect to their findings on the effect of infant age, with a
linear increase in the multi-lab replication study and a finding of stability across infant ages in the
meta-analysis. As we suggest in the paper, this conflict may originate in factors beyond the
underlying construct. Investigators in the meta-analysis had the freedom to tailor their stimuli and
methods to the particular infant age investigated, which may in turn mask age-related changes in
the strength of the IDS preference effect. In contrast, the speech stimuli in the multi-lab replication
study were held uniform across participating laboratories and may have been better suited for
children in older age ranges (ManyBabies Consortium, 2020). The synthesis of these results
underscores the value of being able to generalise across various populations, experimental
methodologies and infant ages, but also highlights the importance of acknowledging the inferential
limitations of evidence synthesis methods (Lakens et al., 2017). Seeing scientific advancement as
an iterative procedure involving data accumulation and theory development empowers us to map
out the diversity of samples in earlier research, scrutinise the possibilities for generalisability, and
point to future directions of research (Fusaroli et al., 2022; Zettersten, Cox, et al., 2023).

In the lecture, we introduced the importance of a cumulative science perspective on scientific enquiry and argued that our explorations need to reflect the systematic exploration of the methodological space. Heterogeneity here plays a key role in the evidence that we have available to us.

Let's imagine a scenario where a synthesis of evidence has produced an effect size estimate of 0.35 [XX; XX], just as in Zettersten, Cox, Bergmann et al. (2024). How can we use this to guide our study of IDS preference?

Let's start by taking the simulation function we made in the previous exercise and adapt it to the new scale of effect sizes:

```{r}
# set up the custom data simulation function
SimulateEffectSizeData <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  beta_0 = 0,   # grand mean
  beta_1 =  0.35,   # effect of category
  omega_0 =  0.05,   # by-item random intercept sd
  tau_0 = 0.1,   # by-subject random intercept sd
  tau_1 =  0.1,   # by-subject random slope sd
  rho = 0.4,   # correlation between intercept and slope
  sigma = 0.5) { # residual (standard deviation)

  items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = omega_0)) %>% 
  mutate(item_id = faux::make_id(nrow(.), "I")) %>%
  mutate(SpeechStyle = recode(Register, "ADS" = -0.5, "IDS" = +0.5))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S"))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())
  
  EffectSizeDataSimulated <- ParameterValues %>%
    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + e_si) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)
}

EffectSizeDataSimulated <- SimulateEffectSizeData()
```

Let's again plot this data and see if it looks correct:

```{r}
EffectSizeDataSimulated <- SimulateEffectSizeData()

meanADSRT <- mean(filter(EffectSizeDataSimulated, Register == "ADS")$LT)

EffectSizeDataSimulated <- EffectSizeDataSimulated %>%
  mutate(LT = LT - meanADSRT)

dat_sim_plot <- EffectSizeDataSimulated %>%
  group_by(subj_id, Register) %>%
  dplyr::summarise(medLT = mean(LT))

ggplot(aes(x = Register, y = medLT, fill = Register), data = dat_sim_plot) + 
  geom_rain(alpha = 0.8, rain.side = "f1x1", id.long.var = "subj_id", point.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42))) + 
  scale_fill_manual(values = c("#FC4E07", "steelblue")) + 
  ggtitle('Effect Size Differences across Speech Styles') + 
  xlab("Speech Style") + 
  ylab('Effect Size') + 
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```

Wonderful, this looks correct! As we saw in the previous exercise sheet, however, our experimental design choices really matter for statistical power. For example, the number of stimulus items (i.e., number of repeated measures per participant) matter hugely, as does number of participants. How can we efficiently explore the multiverse of choices that govern our experimental design?

What if we set up a grid search and allow models to be fit with different parameter values? 

```{r}
run_sims_grid_point <- function(filename_full, trial_n, subj_n) {
  ADS_n = trial_n / 2
  IDS_n = trial_n / 2
  n_subj = subj_n
  
  dataSimulated <- SimulateEffectSizeData(
                        n_subj = n_subj,
                         n_ADS = ADS_n, 
                         n_IDS = IDS_n)
    
  model <- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id),
                data = dataSimulated)

  sim_results <- broom.mixed::tidy(model)
  
  # append the results to a file
  append <- file.exists(filename_full)
  write_csv(sim_results, filename_full, append = append)
  
  # return the tidy table
  sim_results
}

reps <- 2
subj_n <- seq(12, 36, by = 12)
trial_n <- seq(4, 16, by = 4)

param_combinations <- expand.grid(subj_n = subj_n, 
                                  trial_n = trial_n)

for (i in seq_len(nrow(param_combinations))) {
  sim_params <- param_combinations[i, ]
  filename_full <- paste0(here('sims_grid_search/test_grid_search_'),
                          sim_params$subj_n, '_',
                          sim_params$trial_n, '.csv')
  start_time <- Sys.time() # Start time
  sims <- purrr::map_df(1:reps, ~run_sims_grid_point(filename_full = filename_full,
                                                         subj_n = sim_params$subj_n,
                                                         trial_n = sim_params$trial_n))
  end_time <- Sys.time() # End time
  cat("Simulation", i, "Time elapsed:", end_time - start_time, "\n")
}

setwd(here("sims_grid_search"))
file_names <- list.files(pattern = "*.csv")

# read in all CSV files into a list of dataframes
df_list <- purrr::map(file_names, ~{
  df <- read.csv(.x) 
  df$filename <- .x 
  df
  })

df <- purrr::reduce(df_list, dplyr::bind_rows)

df_per_sim <- df %>%
  filter(effect == "fixed") %>%
  filter(term == "SpeechStyle") %>%
  group_by(filename) %>%
  summarise(median_estimate = median(estimate), median_se = median(std.error),
            power = mean(p.value < 0.05))

PowerGridData <- df_per_sim %>%
  mutate(n_subj = sapply(strsplit(filename, "_"), `[`,4),
         n_trial = as.numeric(str_replace(sapply(strsplit(filename, "_"), `[`, 5), pattern = ".csv","")))

ggplot(PowerGridData) +
  geom_point(aes(x = n_subj, y = power)) +
  plot_theme +
  facet_wrap(~n_trial)
```


```{r}
mod_sim <- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id), 
                data = EffectSizeDataSimulated)
summary(mod_sim)
```
