[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop on Data Simulation & Power Analysis",
    "section": "",
    "text": "This website will form the basis of the 2024 ManyBabies workshop on Data Simulation & Power Analysis. Here you can find the two exercise sheets with example code:\nLINK TO EXERCISE SHEET, PART 1 LINK TO EXERCISE SHEET, PART 2\nThis workshop assumes a little literacy in R and linear mixed-effects models, but I have attempted to make these subjects as accessible as possible to new readers in the topic.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File &gt; New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools &gt; Global Options &gt; R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/SimulatingEffectSizes.html",
    "href": "content/SimulatingEffectSizes.html",
    "title": "Modelling Simulated Effect Size Data",
    "section": "",
    "text": "Let’s start by loading the required packages:\n#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages\n#install.packages('pacman')\n#load the required packages:\npacman::p_load(knitr, # rendering of .Rmd file\n               lme4, # model specification / estimation\n               afex, # anova and deriving p-values from lmer\n               broom.mixed, # extracting data from model fits\n               faux, # generate correlated values\n               tidyverse, # data wrangling and visualisation\n               ggridges, #visualisation\n               viridis, # color schemes for visualisation\n               kableExtra, #helps with knitting the .Rmd file\n               cowplot, #visualisation tool to include multiple plots\n               ggrain,\n               dplyr,\n               here)\nset.seed(1234)\nopts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)\n\nplot_theme &lt;- \n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size = 18), \n        legend.position = \"none\", \n        axis.text.x = element_text(size = 13), \n        axis.title.x = element_text(size = 13), \n        axis.text.y = element_text(size = 12), \n        axis.title.y = element_text(size = 13))",
    "crumbs": [
      "Simulating Effect Sizes"
    ]
  },
  {
    "objectID": "content/SimulatingEffectSizes.html#cumulative-science-perspective",
    "href": "content/SimulatingEffectSizes.html#cumulative-science-perspective",
    "title": "Modelling Simulated Effect Size Data",
    "section": "Cumulative Science Perspective",
    "text": "Cumulative Science Perspective\nScience stands on the shoulders of – not giants – but normal human beings who are as susceptible to confirmation and selection biases as everyone else. The limitations of relying on single studies have motivated a push towards methods of synthesising quantitative evidence in the form of meta- and mega-analyses (Koile & Cristia, 2021; Liu & Almeida, 2023; Zettersten et al., 2023). To counter selection biases and obtain an objective picture of a given field, there exist detailed procedures restricting researcher degrees of freedom in the process (Moher et al., 2009; Page et al., 2021). Synthesising evidence from studies in a cumulative approach offers insights to the generalisability and heterogeneity of the construct across a wide variety of experimental designs, participants and stimuli (Cristia et al., 2022; Cruz Blandón et al., 2023; Koile & Cristia, 2021; Tsuji et al., 2014). There are limitations to this process of estimation across studies, such as issues in transparency and reproducibility (Lakens et al., 2016, 2017; Nuijten et al., 2020), considerable errors of extraction (Zettersten, Cox, et al., 2023), reliance on published literature and concomitant bias (Moher et al., 2009; Page et al., 2021), as well as substantial between-study heterogeneity threatening practical interpretation of results (Mathur & VanderWeele, 2019). There are statistical approaches that attempt to correct for publication bias and to account for heterogeneity; however, major concerns about the validity of meta-analytic estimates remain (cf., Kvarven et al., 2020). These concerns primarily revolve around the reliability of meta-analytic estimates rather than the synthesis of insights and exploration of the sampling space of individual studies (Mathur & VanderWeele, 2020; Zettersten, Cox, et al., 2023). A related cumulative method that synthesises findings across multiple individual experiments involves multi-lab replication studies. In such designs, multiple labs conduct replications of original studies by implementing a common experimental protocol to test a research question across sites (ManyBabies Consortium, 2020). By varying experimenter identity and increasing sample diversity, these large-scale studies contribute to a greater likelihood of generalisability and precision (Byers-Heinlein et al., 2021; ManyBabies Consortium, 2020). However, the high degree of uniformity in methodological and analytic implementation can lead to less generalisability across other conditions than a meta-analysis.\nIn a recent study (Zettersten, Cox, et al., 2023), we attempted to reconcile the results from a multi-lab replication study (ManyBabies Consortium, 2020) and a community-augmented metaanalysis on infants’ preference to attend to IDS over ADS. The findings from each source individually provided an incomplete picture of moderators of the IDS preference effect; for example, the two studies differed with respect to their findings on the effect of infant age, with a linear increase in the multi-lab replication study and a finding of stability across infant ages in the meta-analysis. As we suggest in the paper, this conflict may originate in factors beyond the underlying construct. Investigators in the meta-analysis had the freedom to tailor their stimuli and methods to the particular infant age investigated, which may in turn mask age-related changes in the strength of the IDS preference effect. In contrast, the speech stimuli in the multi-lab replication study were held uniform across participating laboratories and may have been better suited for children in older age ranges (ManyBabies Consortium, 2020). The synthesis of these results underscores the value of being able to generalise across various populations, experimental methodologies and infant ages, but also highlights the importance of acknowledging the inferential limitations of evidence synthesis methods (Lakens et al., 2017). Seeing scientific advancement as an iterative procedure involving data accumulation and theory development empowers us to map out the diversity of samples in earlier research, scrutinise the possibilities for generalisability, and point to future directions of research (Fusaroli et al., 2022; Zettersten, Cox, et al., 2023).\nIn the lecture, we introduced the importance of a cumulative science perspective on scientific enquiry and argued that our explorations need to reflect the systematic exploration of the methodological space. Heterogeneity here plays a key role in the evidence that we have available to us.\nLet’s imagine a scenario where a synthesis of evidence has produced an effect size estimate of 0.35 [XX; XX], just as in Zettersten, Cox, Bergmann et al. (2024). How can we use this to guide our study of IDS preference?\nLet’s start by taking the simulation function we made in the previous exercise and adapt it to the new scale of effect sizes:\n\n# set up the custom data simulation function\nSimulateEffectSizeData &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  beta_0 = 0,   # grand mean\n  beta_1 =  0.35,   # effect of category\n  omega_0 =  0.05,   # by-item random intercept sd\n  tau_0 = 0.1,   # by-subject random intercept sd\n  tau_1 =  0.1,   # by-subject random slope sd\n  rho = 0.4,   # correlation between intercept and slope\n  sigma = 0.5) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = omega_0)) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = -0.5, \"IDS\" = +0.5))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\"))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())\n  \n  EffectSizeDataSimulated &lt;- ParameterValues %&gt;%\n    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + e_si) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)\n}\n\nEffectSizeDataSimulated &lt;- SimulateEffectSizeData()\n\nLet’s again plot this data and see if it looks correct:\n\nEffectSizeDataSimulated &lt;- SimulateEffectSizeData()\n\nmeanADSRT &lt;- mean(filter(EffectSizeDataSimulated, Register ==\n    \"ADS\")$LT)\n\nEffectSizeDataSimulated &lt;- EffectSizeDataSimulated %&gt;%\n    mutate(LT = LT - meanADSRT)\n\ndat_sim_plot &lt;- EffectSizeDataSimulated %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(LT))\n\n`summarise()` has grouped output by 'subj_id'. You can override using the\n`.groups` argument.\n\nggplot(aes(x = Register, y = medLT, fill = Register), data = dat_sim_plot) +\n    geom_rain(alpha = 0.8, rain.side = \"f1x1\", id.long.var = \"subj_id\",\n        point.args.pos = list(position = position_jitter(width = 0.04,\n            height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n            height = 0, seed = 42))) + scale_fill_manual(values = c(\"#FC4E07\",\n    \"steelblue\")) + ggtitle(\"Effect Size Differences across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Effect Size\") + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme\n\nWarning: Duplicated aesthetics after name standardisation: alpha\n\n\nWarning: Using the `size` aesthetic with geom_polygon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nWonderful, this looks correct! As we saw in the previous exercise sheet, however, our experimental design choices really matter for statistical power. For example, the number of stimulus items (i.e., number of repeated measures per participant) matter hugely, as does number of participants. How can we efficiently explore the multiverse of choices that govern our experimental design?\nWhat if we set up a grid search and allow models to be fit with different parameter values?\n\nrun_sims_grid_point &lt;- function(filename_full, trial_n, subj_n) {\n    ADS_n = trial_n/2\n    IDS_n = trial_n/2\n    n_subj = subj_n\n\n    dataSimulated &lt;- SimulateEffectSizeData(n_subj = n_subj,\n        n_ADS = ADS_n, n_IDS = IDS_n)\n\n    model &lt;- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 +\n        SpeechStyle | subj_id), data = dataSimulated)\n\n    sim_results &lt;- broom.mixed::tidy(model)\n\n    # append the results to a file\n    append &lt;- file.exists(filename_full)\n    write_csv(sim_results, filename_full, append = append)\n\n    # return the tidy table\n    sim_results\n}\n\nreps &lt;- 2\nsubj_n &lt;- seq(12, 36, by = 12)\ntrial_n &lt;- seq(4, 16, by = 4)\n\nparam_combinations &lt;- expand.grid(subj_n = subj_n, trial_n = trial_n)\n\nfor (i in seq_len(nrow(param_combinations))) {\n    sim_params &lt;- param_combinations[i, ]\n    filename_full &lt;- paste0(here(\"sims_grid_search/test_grid_search_\"),\n        sim_params$subj_n, \"_\", sim_params$trial_n, \".csv\")\n    start_time &lt;- Sys.time()  # Start time\n    sims &lt;- purrr::map_df(1:reps, ~run_sims_grid_point(filename_full = filename_full,\n        subj_n = sim_params$subj_n, trial_n = sim_params$trial_n))\n    end_time &lt;- Sys.time()  # End time\n    cat(\"Simulation\", i, \"Time elapsed:\", end_time - start_time,\n        \"\\n\")\n}\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 1 Time elapsed: 0.6858349 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 2 Time elapsed: 0.2932589 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 3 Time elapsed: 0.273072 \n\n\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 4 Time elapsed: 0.2286398 \nSimulation 5 Time elapsed: 0.2425148 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 6 Time elapsed: 0.284467 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 7 Time elapsed: 0.2374201 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 8 Time elapsed: 0.2598131 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 9 Time elapsed: 0.3116388 \n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 10 Time elapsed: 0.2474611 \n\n\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 11 Time elapsed: 0.2930639 \n\n\nboundary (singular) fit: see help('isSingular')\n\n\nSimulation 12 Time elapsed: 0.303781 \n\nsetwd(here(\"sims_grid_search\"))\nfile_names &lt;- list.files(pattern = \"*.csv\")\n\n# read in all CSV files into a list of dataframes\ndf_list &lt;- purrr::map(file_names, ~{\n    df &lt;- read.csv(.x)\n    df$filename &lt;- .x\n    df\n})\n\ndf &lt;- purrr::reduce(df_list, dplyr::bind_rows)\n\ndf_per_sim &lt;- df %&gt;%\n    filter(effect == \"fixed\") %&gt;%\n    filter(term == \"SpeechStyle\") %&gt;%\n    group_by(filename) %&gt;%\n    summarise(median_estimate = median(estimate), median_se = median(std.error),\n        power = mean(p.value &lt; 0.05))\n\nPowerGridData &lt;- df_per_sim %&gt;%\n    mutate(n_subj = sapply(strsplit(filename, \"_\"), `[`, 4),\n        n_trial = as.numeric(str_replace(sapply(strsplit(filename,\n            \"_\"), `[`, 5), pattern = \".csv\", \"\")))\n\nggplot(PowerGridData) + geom_point(aes(x = n_subj, y = power)) +\n    plot_theme + facet_wrap(~n_trial)\n\n\n\n\n\n\n\n\n\nmod_sim &lt;- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle |\n    subj_id), data = EffectSizeDataSimulated)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(mod_sim)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n   Data: EffectSizeDataSimulated\n\nREML criterion at convergence: 548.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93634 -0.63868 -0.02949  0.69246  2.75595 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj_id  (Intercept) 0.0218   0.1476       \n          SpeechStyle 0.0138   0.1175   0.83\n item_id  (Intercept) 0.0000   0.0000       \n Residual             0.2245   0.4738       \nNumber of obs: 384, groups:  subj_id, 24; item_id, 16\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  0.19428    0.03864 23.00004   5.028 4.35e-05 ***\nSpeechStyle  0.38856    0.05397 23.00004   7.199 2.49e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nSpeechStyle 0.287 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')",
    "crumbs": [
      "Simulating Effect Sizes"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html",
    "href": "content/ExerciseDataSimulation.html",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "",
    "text": "Let’s start with a straightforward example based on MB1. We are interested in the following question: To what extent do infants demonstrate longer looking times to infant-directed speech as opposed to adult-directed speech? We know that a linear mixed-effects model is an appropriate modelling strategy, so we want to conduct a power analysis with this in mind.\nTo give an overview of the simulation task, we will simulate data from a design with crossed random factors of subjects and stimuli, fit a model to the simulated data, and then see whether the resulting sample estimates are similar to the population values we specified when simulating the data. In this study, infant participants are exposed to recordings of infant-directed and adult-directed speech, and we use their looking times as the primary dependent variable. The key question is whether there is any behavioural differences within each participant according to the type of stimulus.\nLet’s start by loading the required packages:\n\n#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages\noptions(repos = \"https://cran.r-project.org/\")\ninstall.packages('pacman')\n\n\nThe downloaded binary packages are in\n    /var/folders/58/tggzh74d3qv8vr4k29xsnp28n39xzy/T//RtmpBtUSCo/downloaded_packages\n\n#load the required packages:\npacman::p_load(knitr, # rendering of .Rmd file\n               lme4, # model specification / estimation\n               afex, # anova and deriving p-values from lmer\n               broom.mixed, # extracting data from model fits\n               faux, # generate correlated values\n               tidyverse, # data wrangling and visualisation\n               ggridges, #visualisation\n               viridis, # color schemes for visualisation\n               kableExtra, #helps with knitting the .Rmd file\n               cowplot, #visualisation tool to include multiple plots\n               ggrain, #visualisation tool for raincloud plots\n               dplyr,\n               here\n               )\n\nset.seed(1234)\nopts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)\n\nplot_theme &lt;- \n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size = 18), \n        legend.position = \"none\", \n        axis.text.x = element_text(size = 13), \n        axis.title.x = element_text(size = 13), \n        axis.text.y = element_text(size = 12), \n        axis.title.y = element_text(size = 13))",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#a-description-of-the-example",
    "href": "content/ExerciseDataSimulation.html#a-description-of-the-example",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "",
    "text": "Let’s start with a straightforward example based on MB1. We are interested in the following question: To what extent do infants demonstrate longer looking times to infant-directed speech as opposed to adult-directed speech? We know that a linear mixed-effects model is an appropriate modelling strategy, so we want to conduct a power analysis with this in mind.\nTo give an overview of the simulation task, we will simulate data from a design with crossed random factors of subjects and stimuli, fit a model to the simulated data, and then see whether the resulting sample estimates are similar to the population values we specified when simulating the data. In this study, infant participants are exposed to recordings of infant-directed and adult-directed speech, and we use their looking times as the primary dependent variable. The key question is whether there is any behavioural differences within each participant according to the type of stimulus.\nLet’s start by loading the required packages:\n\n#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages\noptions(repos = \"https://cran.r-project.org/\")\ninstall.packages('pacman')\n\n\nThe downloaded binary packages are in\n    /var/folders/58/tggzh74d3qv8vr4k29xsnp28n39xzy/T//RtmpBtUSCo/downloaded_packages\n\n#load the required packages:\npacman::p_load(knitr, # rendering of .Rmd file\n               lme4, # model specification / estimation\n               afex, # anova and deriving p-values from lmer\n               broom.mixed, # extracting data from model fits\n               faux, # generate correlated values\n               tidyverse, # data wrangling and visualisation\n               ggridges, #visualisation\n               viridis, # color schemes for visualisation\n               kableExtra, #helps with knitting the .Rmd file\n               cowplot, #visualisation tool to include multiple plots\n               ggrain, #visualisation tool for raincloud plots\n               dplyr,\n               here\n               )\n\nset.seed(1234)\nopts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)\n\nplot_theme &lt;- \n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size = 18), \n        legend.position = \"none\", \n        axis.text.x = element_text(size = 13), \n        axis.title.x = element_text(size = 13), \n        axis.text.y = element_text(size = 12), \n        axis.title.y = element_text(size = 13))",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#establishing-the-data-generating-parameters",
    "href": "content/ExerciseDataSimulation.html#establishing-the-data-generating-parameters",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Establishing the data-generating parameters",
    "text": "Establishing the data-generating parameters\nThe first thing to do is to set up the parameters that govern the process we assume to give rise to the data: the data-generating process. Let’s start by defining the number of stimulus items that each infant participant will be exposed to. Because infants are not the most patient of participants, perhaps a realistic study design would allow researchers to expose infants to 8 recordings of IDS and 8 recordings of ADS. Let’s say that a realistic sample size for our laboratory would be around 25 participants. This implies a total of 400 obsevations in this study (i.e., 8 + 8 recording stimuli for each of the 25 children). This is a within-subjects, between-items study; that is, each and every subject receives both IDS and ADS stimuli (within-subject), but each stimulus is either IDS or ADS (between-items).\nNow that we have an overview of a dataset that would make sense for our experimental design, let’s start to make some realistic and informed choices for parameters that would make sense to build a statistical model. In the following sections, we will build up the parameters for a mixed-effects statistical model of the following type, as described in the lecture:\nDV = β₀ + β₁ * SpeechStyle + u₀ⱼ^(item) + u₀ᵢ^(subj) + (u₁ᵢ^(subj) * SpeechStyle) + ε\nAccording to this formula, the dependent variable for each subject and item is defined as sum of an intercept term 𝛽0, which in this example is the grand mean reaction time for the population of stimuli, plus 𝛽1, the mean RT difference ADS and IDS stimuli, plus random noise. To make 𝛽0 equal the grand mean and 𝛽1 equal the mean IDS minus the mean ADS RT, we will code the SpeechStyle category variable -0.5 for ADS and +0.5 for IDS.\nThe parameters 𝛽0 and 𝛽1 are fixed effects: they characterize the population of events in which a typical subject encounters a typical stimulus. Thus, we set the mean RT for a “typical” infant participant encountering a “typical” stimulus to 800 ms, and assume that looking times are typically 2000ms longer for IDS as opposed to ADS.\nTo summarize, we established a reasonable statistical model underlying the data having the form:\nThe response time for subject 𝑠on item 𝑖, 𝑅𝑇𝑠𝑖, is decomposed into a population grand mean 𝛽0, a by-subject random intercept 𝑇0𝑠, a by-item random intercept 𝑂0𝑖, a fixed slope 𝛽1, a by-subject random slope 𝑇1𝑠, and a trial-level residual 𝑒𝑠𝑖. Our data-generating process is fully determined by seven population parameters, all denoted by Greek letters: 𝛽0, 𝛽1, 𝜏0, 𝜏1, 𝜌, 𝜔0, and 𝜎. In the next section we will apply this data-generating process to simulate the sampling of subjects, items, and trials (encounters).\nThe output of the above function tells us that we now have three sources of variation in the model:\n\nthe usual standard deviation of the residuals (i.e., ‘sigma’),\nthe standard deviation of the population of by-subject varying intercepts (i.e., ‘Intercept’), and\nthe standard deviation of the population of by-subject varying slopes (i.e., ‘RegisterIDS’).\n\nThe latter two sources of variation provide one of the most essential features of multi-level models: partial pooling, as we discussed in the lecture.\nAccording to the formula, response 𝑅𝑇𝑠𝑖for subject 𝑠 and item 𝑖 is defined as sum of an intercept term 𝛽0, which in this example is the grand mean reaction time for the population of stimuli, plus 𝛽1, the mean RT difference between ingroup and outgroup stimuli, plus random noise 𝑒𝑠𝑖. To make 𝛽0 equal the grand mean and 𝛽1 equal the mean outgroup minus the mean ingroup RT, we will code the item category variable 𝑋𝑖as -.5 for the ingroup category and +.5 for the outgroup category.\nNow that we have an appropriate structure for our simulated dataset, we need to generate the RT values. For this, we need to establish an underlying statistical model. In this and the next section, we will build up a statistical model step by step, defining variables in the code as we go along that reflect our choices for parameters. For convenience, Table @ref(tab:paramdef) lists all of the variables in the statistical model and their associated variable names in the code.\nLet us start with a basic model and build up from there. We want a model of RT for subject 𝑠 and item 𝑖that looks something like:\nIn the model formula, we use Greek letters (𝛽0, 𝛽1) to represent population parameters that are being directly estimated by the model. In contrast, Roman letters represent the remaining variables: observed variables whose values are determined by sampling (e.g., 𝑅𝑇𝑠𝑖, 𝑇0𝑠, 𝑒𝑠𝑖) or fixed by the experiment design (𝑋𝑖).\nAlthough this model is incomplete, we can go ahead and choose parameters for 𝛽0 and 𝛽1. For this example, we set a grand mean of 800 ms and a mean difference of 50 ms. You will need to use disciplinary expertise and/or pilot data to choose these parameters; by the end of this tutorial you will understand how to extract those parameters from an analysis.\nLet’s start by setting the fixed-effects parameters; that is, the overall intercept (ADS) and the effect of speech style (IDS). We can be informed about these choices from what we know about looking times in infant expperiments. We could imagine a looking time of around 9 seconds (i.e., 9000ms) for ADS stimuli and for infants to exhibit longer looking times towards IDS stimuli, maybe around 11 seconds (i.e., 11000 ms), or 2000 ms longer looking times than that to ADS. Let’s code this explicitly as parameter values in for our simulation:\n\n# set fixed effect parameters\nbeta_0 &lt;- 9000  # intercept; i.e., the grand mean, ADS \nbeta_1 &lt;- 2000  # slope; i.e, effect of IDS \n\nWe also need to add some variability around these values, as we can’t expect each and every infant in the experiment to have exactly the same baseline reaction to the ADS stimuli, and we can’t expect each stimulus to have exactly the same effect. We would expect variability in both subject reactions and item effects. Let’s code this as standard deviation of by-subject random intercept and by-item random intercept sd.\n\n# set random effect parameters\ntau_0 &lt;- 3000  # by-subject random intercept sd\nomega_0 &lt;- 1000  # by-item random intercept sd\n\nWe also can’t expect the effect of IDS stimuli to be the same for every infant; that is, some infants’ attention may be more grabbed by IDS stimuli than others, and we therefore need to add subject-specific variability for the effect of IDS on looking times. Let’s code this as the tau_1 parameter. Here we can also add a crucial component of multi-level models, which is the correlation matrix. There may be patterns in the data between those infants who have a high looking time baseline in ADS and the effects of IDS, and we therefore have to add a correlation matrix to account for these patterns. Here, we specify that there is a weak correlation between the varying intercepts and varying slopes of infant participants. We also need a residual error term to capture unexplained sources of variability:\n\n# set more random effect and error parameters\ntau_1 &lt;- 1000  # by-subject random slope sd\nrho &lt;- 0.2  # correlation between intercept and slope\nsigma &lt;- 500  # residual (error) sd",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#define-parameters-for-observations",
    "href": "content/ExerciseDataSimulation.html#define-parameters-for-observations",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Define parameters for observations",
    "text": "Define parameters for observations\nNow that we have our initial parameters in place, it’s time to start defining parameters related to the number of observations. In this example, we will simulate data from a typical IDS preference experiment, with 24 infants responding to 8\n20 subjects responding to 25 ingroup faces and 25 outgroup faces. There are no between-subject factors, so we can set n_subj to 100. We set n_ingroup and n_outgroup to the number of stimulus items in each condition.\n\n# set number of subjects and items\nn_subj &lt;- 24  # number of subjects\nn_ADS &lt;- 8  # number of ADS stimuli\nn_IDS &lt;- 8  # number of IDS stimuli",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-stimulus-items",
    "href": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-stimulus-items",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Simulate the sampling of stimulus items",
    "text": "Simulate the sampling of stimulus items\nNow it’s time to create a dataset that lists each item, which speech style it is in and its varying properties. Note that the code refers back to the omega_0 parameter that we specified above for the varying item intercept. For the varying item variable, we are going to use the faux package to assign a unique identifer to each of the 16 speech stimuli and designate whether the stimuli are ADS or IDS, with the first 8 being ADS and the next 8 being IDS. We sample the values of 𝑂0𝑖 from a normal distribution using the rnorm() function, with a mean of 0 and SD of 𝜔0.\n\n# simulate a sample of items\n# total number of items = n_ADS + n_IDS\nitems &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = omega_0)\n) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = -0.5, \"IDS\" = +0.5))\n\n#let's have a look at what this has produced:\nglimpse(items)\n\nRows: 16\nColumns: 4\n$ Register    &lt;chr&gt; \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"A…\n$ O_0i        &lt;dbl&gt; -1207.06575, 277.42924, 1084.44118, -2345.69770, 429.12469…\n$ item_id     &lt;chr&gt; \"I01\", \"I02\", \"I03\", \"I04\", \"I05\", \"I06\", \"I07\", \"I08\", \"I…\n$ SpeechStyle &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -0.5, -0.5, -0.5, …\n\n\nLet us introduce a numeric predictor to represent what category each stimulus item 𝑖 appears in (i.e., for the 𝑋𝑖 in our model). Since we predict that looking times to ADS stimuli will be shorter than IDS stimuli, we set ADS to be -0.5 and IDS to be +0.5. This is what is known as contrast coding, and we will later multiply this effect-coded factor by the fixed effect of category to simulate data where the ADS stimuli on average generate looking times of -1000 ms different from the grand mean, while the IDS stimuli are on average 1000 ms different from the grand mean.\nIn R, most regression procedures can handle two-level factors like category as predictor variables. By default, the procedure will create a new numeric predictor that codes one level of the factor as zero and the other as one. Why not just use the defaults? The short explanation is that the default of 0, 1 coding is not well-suited to the kinds of factorial experimental designs often found in psychology and related fields. For the current example, using the default coding for the 𝑋 predictor would change the interpretation of 𝛽0: instead of the grand mean, it would reflect the mean for the group coded as zero. One could change the default, but we feel it is better to be explicit in the code about what values are being used.",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-subjects",
    "href": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-subjects",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Simulate the sampling of subjects",
    "text": "Simulate the sampling of subjects\nNow we will simulate the sampling of individual subjects, resulting in a table listing each subject and their two correlated varying effects (i.e., intercepts and slopes). This will be slightly more complicated that what we just did, because we cannot simply sample the 𝑇0𝑠 values from a univariate distribution using rnorm() independently from the 𝑇1𝑠 values. Instead, we must sample ⟨𝑇0𝑠,𝑇1𝑠⟩ pairs — one pair for each subject—from a bivariate normal distribution. One way to sample from a bivariate distribution would be to use the function rnorm_multi() from the faux package (DeBruine 2020), which generates a table of n simulated values from a multivariate normal distribution by specifying the means (mu) and standard deviations (sd) of each variable, plus the correlations (r), which can be either a single value (applied to all pairs), a correlation matrix, or a vector of the values in the upper right triangle of the correlation matrix.\n\n# simulate a sample of subjects\n\n# sample from a multivariate random distribution \nsubjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\"))",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#simulate-trials-encounters",
    "href": "content/ExerciseDataSimulation.html#simulate-trials-encounters",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Simulate trials (encounters)",
    "text": "Simulate trials (encounters)\nSince all subjects respond to all items, we can set up a table of trials by making a table with every possible combination of the rows in the subject and item tables using the tidyverse function crossing(). Each trial has random error associated with it, reflecting fluctuations in trial-by-trial performance due to unknown factors; we simulate this by sampling values from a normal distribution with a mean of 0 and SD of sigma.\n\n# cross subject and item IDs; add an error term nrow(.) is\n# the number of rows in the table\nParameterValues &lt;- crossing(subjects, items) %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#adding-it-all-together",
    "href": "content/ExerciseDataSimulation.html#adding-it-all-together",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Adding it all together",
    "text": "Adding it all together\nNow we have specified the parameters in ParameterValues, we are ready to add up everything together to create the response variable (i.e., infant looking times in milliseconds). To be more specific, we calculate the response variable, looking time, by adding together:\n\nthe grand intercept (beta_0),\neach subject-specific random intercept (T_0s),\neach item-specific random intercept (O_0i),\neach sum of the category effect (beta_1) and the random slope (T_1s), multiplied by the numeric predictor (SpeechStyle), and\neach residual error (e_si).\n\nAfter this we will use dplyr::select() to keep the columns we need. This will create the structure that we set as our goal at the start of this exercise, with the additional column SpeechStyle, which we will keep to use in the estimation process, described in the next section.\n\n# calculate the response variable\ndat_sim &lt;- ParameterValues %&gt;%\n    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle +\n        e_si) %&gt;%\n    mutate(LT = LT + rexp(nrow(.), rate = 3e-04)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)\n\nLet’s have a look at what the data we have generated looks like:\n\ndat_sim %&gt;%\n    ggplot() + geom_density(aes(x = LT, fill = as.factor(SpeechStyle)),\n    alpha = 0.3) + xlim(c(-5000, 40000)) + plot_theme\n\n\n\n\n\n\n\ndat_sim_plot &lt;- dat_sim %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(LT))\n\n`summarise()` has grouped output by 'subj_id'. You can override using the\n`.groups` argument.\n\nggplot(aes(x = Register, y = medLT, fill = Register), data = dat_sim_plot) +\n    geom_rain(alpha = 0.8, rain.side = \"f1x1\", id.long.var = \"subj_id\",\n        point.args.pos = list(position = position_jitter(width = 0.04,\n            height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n            height = 0, seed = 42))) + scale_fill_manual(values = c(\"#FC4E07\",\n    \"steelblue\")) + ggtitle(\"Looking Time Differences across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Looking Time (ms)\") + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme\n\nWarning: Duplicated aesthetics after name standardisation: alpha\n\n\nWarning: Using the `size` aesthetic with geom_polygon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nWonderful, so now we have simulated the dataset with relevant properties that we can use to in sophisticated linear mixed effects models! To make it easier to try out different parameters or to generate many datasets for the purpose of power analysis, we can put all of the code above into a custom function. Set up the function to take all of the parameters we set above as arguments. We’ll set the defaults to the values we used, but you can choose your own defaults. The code below is just all of the code above, condensed a bit. It returns one dataset with the parameters we specified.\n\n# set up the custom data simulation function\nSimulateData &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  beta_0 = 9000,   # grand mean\n  beta_1 =  3000,   # effect of category\n  omega_0 =  1000,   # by-item random intercept sd\n  tau_0 = 2000,   # by-subject random intercept sd\n  tau_1 =  2500,   # by-subject random slope sd\n  rho = 0.5,   # correlation between intercept and slope\n  sigma = 200) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = omega_0)) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = -0.5, \"IDS\" = +0.5))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\"))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())\n  \n  dat_sim &lt;- ParameterValues %&gt;%\n    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + e_si) %&gt;%\n    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)\n}\n\nNow we can generate a dataset with the default parameters using my_sim_data() or, for example, a dataset with 500 subjects and no effect of category using my_sim_data(n_subj = 500, beta_1 = 0).\n\ndata &lt;- SimulateData(n_subj = 500, beta_1 = 2000)\n\ndat_sim_plot &lt;- data %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(LT))\n\n`summarise()` has grouped output by 'subj_id'. You can override using the\n`.groups` argument.\n\nggplot(aes(x = Register, y = medLT, fill = Register), data = dat_sim_plot) +\n    geom_rain(alpha = 0.8, rain.side = \"f1x1\", id.long.var = \"subj_id\",\n        point.args.pos = list(position = position_jitter(width = 0.04,\n            height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n            height = 0, seed = 42))) + scale_fill_manual(values = c(\"#FC4E07\",\n    \"steelblue\")) + ggtitle(\"Looking Time Differences across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Looking Time (ms)\") + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme\n\nWarning: Duplicated aesthetics after name standardisation: alpha",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#modelling-the-simulated-data",
    "href": "content/ExerciseDataSimulation.html#modelling-the-simulated-data",
    "title": "Exercise Sheet 1, Data Simulation",
    "section": "Modelling the Simulated Data",
    "text": "Modelling the Simulated Data\nLet’s think about how we want to run a generalised linear mixed-effects model of the data. For a varying intercepts, varying slopes model for each subject, we could run the following model with the lmer syntax as follows:\n\nLT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n\nIn this model, LT is the response variable; 1 corresponds to the grand intercept (beta_0); SpeechStyle is the predictor for the ADS/IDS manipulation for item i; (1 | item_id) specifies a by-subject random intercept (O_0i); (1 + SpeechStyle | subj_id) specifies a subject-specific random intercept (T_0s) plus the subject-specific random slope of SpeechStyle (T_1s). The error term (e_si) is automatically included in all models, so is left implicit.\nThe terms in parentheses with the “pipe” separator (|) define the random effects structure. For each of these bracketed terms, the left-hand side of the pipe names the effects you wish to allow to vary and the right hand side names the variable identifying the levels of the random factor over which the terms vary (e.g., subjects or items). The first term, (1 | item_id) allows the intercept (1) to vary over the random factor of items (item_id). This is an instruction to estimate the parameter underlying the O_0i values, namely omega_0. The second term, (1 + X_i | subj_id), allows both the intercept and the effect of category (coded by X_i) to vary over the random factor of subjects (subj_id). It is an instruction to estimate the three parameters that underlie the T_0s and T_1s values, namely tau_0, tau_1, and rho.\nGiven that looking time data tends to be right-skewed, we can model the data using a logarithmic link function to take this into account. The GLMM would look as follows. We will use the summary() function to view the results.\n\ndataSimulated &lt;- SimulateData()\n\nmodel &lt;- glmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle |\n    subj_id), data = dataSimulated, family = Gamma(link = \"log\"))\n\nsummary(model)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Gamma  ( log )\nFormula: LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n   Data: dataSimulated\n\n     AIC      BIC   logLik deviance df.resid \n  7357.5   7385.2  -3671.8   7343.5      377 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6789 -0.6401 -0.1412  0.4258  3.9262 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj_id  (Intercept) 0.009583 0.09789      \n          SpeechStyle 0.039367 0.19841  0.24\n item_id  (Intercept) 0.007222 0.08498      \n Residual             0.083845 0.28956      \nNumber of obs: 384, groups:  subj_id, 24; item_id, 16\n\nFixed effects:\n            Estimate Std. Error t value Pr(&gt;|z|)    \n(Intercept)  9.38583    0.05888 159.406   &lt;2e-16 ***\nSpeechStyle  0.22297    0.11876   1.877   0.0605 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nSpeechStyle 0.155 \n\n\nGreat, so now we have a way of generating data for our research question, and we have a clear idea of how we want to model the data. Now it’s time to run the actual power analysis. The way we do this is to specify an effect, run a model and count how many of the models show significant effects (i.e., the ground truth).\n\n# simulate, analyze, and return a table of parameter\n# estimates\nsingle_run &lt;- function(...) {\n    # ... is a shortcut that forwards any arguments to\n    # my_sim_data(), the function created above\n    dataSimulated &lt;- SimulateData()\n\n    model &lt;- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 +\n        SpeechStyle | subj_id), data = dataSimulated)\n    # family = gaussian(link = 'log'))\n\n    broom.mixed::tidy(model)\n}\nsingle_run()\n\n# A tibble: 7 × 8\n  effect   group    term            estimate std.error statistic    df   p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)      1.31e+4      607.     21.6   28.0  5.53e-19\n2 fixed    &lt;NA&gt;     SpeechStyle      4.27e+3      912.      4.68  27.2  7.06e- 5\n3 ran_pars subj_id  sd__(Intercept)  2.59e+3       NA      NA     NA   NA       \n4 ran_pars subj_id  cor__(Intercep…  5.84e-1       NA      NA     NA   NA       \n5 ran_pars subj_id  sd__SpeechStyle  3.38e+3       NA      NA     NA   NA       \n6 ran_pars item_id  sd__(Intercept)  9.32e+2       NA      NA     NA   NA       \n7 ran_pars Residual sd__Observation  3.63e+3       NA      NA     NA   NA       \n\n\nNow we have a function to generate data, run our model and spit out the results! Now it’s time to repeat a few hundred times, so that we can calculate how much power we have with our given parameters!\n\n# run simulations and save to a file\nn_runs &lt;- 100 # use at least 100 to get stable estimates\nsims &lt;- purrr::map_df(1:n_runs, ~ single_run())\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nwrite_csv(sims, \"sims.csv\")\n\n# read saved simulation data\nsims &lt;- read_csv(\"sims.csv\", col_types = cols(\n  # makes sure plots display in this order\n  group = col_factor(ordered = TRUE),\n  term = col_factor(ordered = TRUE)\n  ))\n\nsims %&gt;%\n  filter(effect == \"fixed\") %&gt;%\n  dplyr::select(term, estimate, p.value)\n\n# A tibble: 200 × 3\n   term        estimate  p.value\n   &lt;fct&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)   12665. 8.49e-21\n 2 SpeechStyle    3050. 1.01e- 4\n 3 (Intercept)   12491. 5.50e-21\n 4 SpeechStyle    2612. 2.00e- 3\n 5 (Intercept)   12362. 6.28e-19\n 6 SpeechStyle    3138. 4.83e- 5\n 7 (Intercept)   11495. 1.03e-20\n 8 SpeechStyle    2145. 6.67e- 3\n 9 (Intercept)   12797. 1.84e-21\n10 SpeechStyle    5282. 1.48e- 7\n# ℹ 190 more rows\n\n# calculate mean estimates and power for specified alpha\nalpha &lt;- 0.05\n\nsims %&gt;% \n  filter(effect == \"fixed\") %&gt;%\n  group_by(term) %&gt;%\n  dplyr::summarize(\n    mean_estimate = mean(estimate),\n    mean_se = mean(std.error),\n    power = mean(p.value &lt; alpha),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  term        mean_estimate mean_se power\n  &lt;fct&gt;               &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)        12440.    502.  1   \n2 SpeechStyle         3053.    793.  0.94\n\n\nAfter running this model a couple of hundred times, we can converge on values for power that are &gt; .80 - perfect for our little study on the IDS preference effect. However, we should note the limitations in this approach. There are a bunch of assumptions that went into specifying the different parameters, and we need a way to grid search through values to put the power results into perspective. We turn to this problem in the second exercise sheet, but first, let’s make sure that we’ve understood the principles behind data simulation and power analysis.\nExercise 1: Let’s explore the effect of repeated measures on power. Try to run a power analysis with each subject receiving two items and another power analysis with each subject receiving 15 items. What happens to the estimate of statistical power?\nWrite your answer here:\n\nExercise 2: We might expect the IDS preference effect to change with infant age, such that older infants listen longer to IDS over ADS than younger infants. How would you add a positive interaction effect of (cross-sectional) age as a predictor to the model (hint: it involves randomly sampling age for each child and adding an effect to the simulation code and model)?",
    "crumbs": [
      "Introduction to Data Simulation"
    ]
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  }
]