[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop on Data Simulation & Power Analysis",
    "section": "",
    "text": "Welcome! This website will form the basis of the 2024 ManyBabies workshop on Data Simulation & Power Analysis. Before we start, I would like to emphasise that this workshop has grown out of discussion with lots of different people and is a true collaborative effort, which accords nicely with the general philosophy of ManyBabies projects. The approach to data simulation and power analysis explored here is closely associated with the data analysis team on the ManyBabies5 project (https://manybabies.org/MB5/). This series of meetings among researchers on the data analysis team was a fundamentally exploratory process being guided by a what-if mindset.\nFor example, how do predictors with two or three levels impact the power to detect an effect on infant looking times? How does variability in individual labs come to affect generalisability and statistical power? What is the optimal balance between various pracical constraints (e.g., an upper bound on the number of stimulus items that infants can attend to) and statistical inference (e.g., how much of a decrease in power are we willing to accept based on the above constraints)? Can these results inform the experimental design in some way and improve chances of replicability?\nThis workshop assumes a little literacy in R and linear mixed-effects models, but I have attempted to make these subjects as accessible as possible, even without knowledge of these topics. If you are interested in gaining hands-on pracical experience with the code, then feel free to download the following .Rmd files with the code, so that you can get a better idea of what each code snippet does and can manipulate them according to your own needs and studies.\nLINK TO EXERCISE SHEET, PART 1 LINK TO EXERCISE SHEET, PART 2 *LINK TO EXERCISE SHEET, PART 3\nI hope that this will be fun experience and useful exploration of data simulation, power analysis, statistical modelling and programming, and if you have any questions, big or small, feel free to contact me on chris[dot]mm[dot]cox@gmail.com.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/FurtherResources.html",
    "href": "content/FurtherResources.html",
    "title": "Further Resources",
    "section": "",
    "text": "Thank you so much to the Teaching, Training and Open Science Committee at ManyBabies for supporting the creation of this workshop! I take full responsibility for all mistakes. I have taken inspiration from the following ressources and would recommend them for everyone who is interested in this topic:\n\nDeBruine, L. M., & Barr, D. J. (2021). Understanding mixed-effects models through data simulation. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920965119.\nKoch, T., Pargent, F., Kleine, A. K., Lermer, E., & Gaube, S. (2023). A Tutorial on Tailored Simulation-Based Power Analysis for Experimental Designs with Generalized Linear Mixed Models. PsyArxiv: https://osf.io/preprints/psyarxiv/rpjem/.\nLakens, D., & Caldwell, A. R. (2021). Simulation-based power analysis for factorial analysis of variance designs. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920951503.\nhttps://rpsychologist.com/cohend/",
    "crumbs": [
      "Further Resources"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html",
    "href": "content/ModellingTheData.html",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "We know that science stands on the shoulder of – not giants – but normal human beings who are as susceptible to confirmation and selection biases as everyone else. And up until now, we have relied on our intuitions about infant looking times based solely on our experience with conducting infant experiments. In the first exercise, it quickly became clear that we relied heavily on our own intuitions about what we know about the domain of infant looking times. In this section, we consider how to improve our data simulation process by capitalising on cumulative science efforts, such as meta-analyses and multi-lab replication studies. Seeing scientific advancement as an iterative procedure involving data accumulation and synthesis really empowers us to map out the diversity of samples in earlier research, scrutinise the possibilities for generalisability, and point to future directions of research.\nFor the IDS preference effect, for example, we can thank the ManyBabies community for conducting both a multi-lab replication study and a community-augmented meta-analysis on infants’ preference to attend to IDS over ADS (ManyBabies Consortium, 2020; Zettersten, Cox, et al., 2023). By synthesising data across such a wide variety of experimental designs, participants and stimuli, we now have a fairly good estimate of the overall magnitude of the IDS preference effect. Both sources of evidence converge on an effect size of 0.35 with 95% CI of [0.16; 0.47].\nWhen calculating power, we need to leverage results from these cumulative science efforts so that we can engage in iterative development of our estimates and theories. This section delves into the realm of effect sizes and teaches you how to implement a power analysis based on an effect size estimate.\n\n\nLet’s continue with our IDS preference example and take inspiration from the ManyBabies1 estimate (https://doi.org/10.1177/2515245919900809) to think about how we would simulate data for a new experimental study on the IDS preference effect. Because we are still interested in a within-subjects, between-items study, we can rely on the simulation function from the previous page. All we have to do is to adapt the simulation function so that it suits the new scale of effect sizes. Effect size is a simple way of quantifying the size of the difference between two groups. It allows us to move beyond the “Does it work?” question to “How well does it work in a range of contexts?”. In short, the effect size is based on the standardised mean difference, and quantifies the difference between two means in terms of standard deviation units; an effect size of 0.35 thus implies a difference between the two groups of 0.35 standard deviations.\n\n\n\nLet’s adapt our simulation function from previous pages to the new scale of effect sizes and call it SimulateEffectSizeData(). Following ManyBabies Consortium (2020), a positive effect size denotes longer looking times to IDS stimuli over ADS stimuli, and an effect size of 0 denotes no preference for either speech style (i.e., similar looking times to ADS and IDS stimuli).\n\n# set up the custom data simulation function\nSimulateEffectSizeData &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  mean_intercept = 0,   # grand mean\n  mean_slope =  0.35,   # effect of category\n  item_varyingintercept =  0.05,   # by-item random intercept sd\n  item_varyingslope =  0.05,   # by-item random intercept sd\n  subject_varyingintercept = 0.1,   # by-subject random intercept sd\n  subject_varyingslope =  0.1,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 0.3) { # residual (standard deviation)\n\n    items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  faux::rnorm_multi(\n  n = n_ADS + n_IDS, \n  mu = 0, # means for random effects are always 0\n  sd = c(item_varyingintercept, item_varyingslope),\n  r = rho,\n  varnames = c(\"item_sd\", \"item_slope_sd\"))) %&gt;%\n    mutate(item_id = faux::make_id(n_ADS + n_IDS, \"I\")) %&gt;%\n    mutate(SpeechStyle = recode(Register, \"ADS\" = 0, \"IDS\" = 1))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(subject_varyingintercept, subject_varyingslope), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"subject_sd\", \"subject_slope_sd\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\"))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())\n  \n  ParameterValues %&gt;%\n    mutate(EF = mean_intercept + subject_sd + item_sd + (mean_slope + subject_slope_sd + item_varyingslope) * SpeechStyle + e_si) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, EF)\n}\n\n\nEffectSizeDataSimulated &lt;- SimulateEffectSizeData()\nEffectSizeDataSimulated %&gt;%\n    ggplot() + geom_density(aes(EF, fill = Register), alpha = 0.8) +\n    geom_vline(xintercept = 0.35, linetype = 3) + geom_vline(xintercept = 0,\n    linetype = 3) + xlim(c(-5 * 0.35, 5 * 0.35)) + ggtitle(\"IDS Preference Effect in Effect Sizes\") +\n    plot_theme + scale_fill_brewer(palette = \"Dark2\") + theme(axis.title.y = element_blank(),\n    axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nEffectSizeDataSimulated %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(EF), .groups = \"drop\") %&gt;%\n    ggplot(aes(x = Register, y = medLT, fill = Register)) + geom_rain(alpha = 0.8,\n    rain.side = \"f1x1\", id.long.var = \"subj_id\", point.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42))) + scale_fill_brewer(palette = \"Dark2\") +\n    ggtitle(\"Subject-Level Differences across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Effect Size\") + plot_theme\n\n\n\n\n\n\n\n\n\n\n\nLet’s think about how we want to run a linear mixed-effects model of the data. For a varying-intercepts, varying-slopes model for each subject, we could run the following model with the lmer syntax as follows:\n\nEF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n\nIn this model, EF is the effect size response variable; 1 corresponds to the average intercept; SpeechStyle is the predictor for the ADS/IDS manipulation for item i; (1 | item_id) specifies a by-subject random intercept (O_0i); (1 + SpeechStyle | subj_id) specifies a subject-specific random intercept (T_0s) plus the subject-specific random slope of SpeechStyle (T_1s). The error term (e_si) is automatically included in all models, so is left implicit in the above formula. The terms in parentheses with the “pipe” separator (|) define the random effects structure. For each of these bracketed terms, the left-hand side of the pipe names the effects you wish to allow to vary and the right hand side names the variable identifying the levels of the random factor over which the terms vary (e.g., subjects or items). The first term, (1 | item_id) allows the intercept (1) to vary over the random factor of items (item_id). This is an instruction to estimate the parameter underlying the O_0i values, namely omega_0. The second term, (1 + X_i | subj_id), allows both the intercept and the effect of category (coded by X_i) to vary over the random factor of subjects (subj_id). It is an instruction to estimate the three parameters that underlie the T_0s and T_1s values, namely tau_0, tau_1, and rho.\n\ndataSimulated &lt;- SimulateEffectSizeData()\n\nmodel &lt;- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle |\n    subj_id), data = dataSimulated)\n\nsummary(model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n   Data: dataSimulated\n\nREML criterion at convergence: 191.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.92232 -0.63935 -0.01405  0.65895  2.81429 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj_id  (Intercept) 0.011522 0.10734      \n          SpeechStyle 0.006000 0.07746  0.98\n item_id  (Intercept) 0.005294 0.07276      \n Residual             0.082007 0.28637      \nNumber of obs: 384, groups:  subj_id, 24; item_id, 16\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept) -0.02371    0.03961 20.33685  -0.599    0.556    \nSpeechStyle  0.41100    0.04927 14.95746   8.341 5.23e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nSpeechStyle -0.385\n\n\nTry to run this a couple of times with new simulated data and see how the estimates and significance measures change. In the following code blocks, we will automatise this process and run the power analysis proper!\n\n\n\nNow we have two essential components to perform a simulation-based power analysis: i) a code pipeline to generate data for our research question and ii) a clear idea of how we want to model the data. Now it’s time to run the actual power analysis. The way we do this is to specify an effect, run 100s of models and count how many of the models show significant effects. To accomplish this, we can write a new function that combines a modelling component into our previous SimulateEffectSizeData() function. That is, if we include both the simulation and modelling of the data in one function, we can simplify the process of performing a power analysis. We will call this function SimulateAndModelEFData(), and we will use broom.mixed::tidy(model) to obtain a dataframe with relevant results from the model and write them to a .csv-file.\n\n# simulate, analyze, and return a table of parameter\n# estimates\nSimulateAndModelEFData &lt;- function(...) {\n    # simulate EF data function\n    dataSimulated &lt;- SimulateEffectSizeData()\n\n    # model EF data\n    model &lt;- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 +\n        SpeechStyle | subj_id), data = dataSimulated)\n    # write to a dataframee\n    broom.mixed::tidy(model)\n}\n\n\n\n\nNow we have a function to generate data, run our model and spit out the results! Now it’s time to repeat a few hundred times, so that we can calculate how much power we have with our given parameters. We are going to use map_df() to run the simulation and modelling function 500 times and write it to a .csv file.\nIf it ran correctly, it should have produced a .csv file with model results from each new simulation of data. Let’s read in the results and have a look at what they say!\n\n# read saved simulation data\nsims &lt;- read_csv(\"EFsimulations.csv\", show_col_types = FALSE) %&gt;%\n    dplyr::select(term, estimate, std.error, p.value)\n\nsims %&gt;%\n    group_by(term) %&gt;%\n    dplyr::summarize(mean_estimate = mean(estimate), mean_se = mean(std.error),\n        power = mean(p.value &lt; 0.05), .groups = \"drop\")\n\n# A tibble: 6 × 4\n  term                         mean_estimate mean_se  power\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)                        0.00207  0.0343  0.036\n2 SpeechStyle                        0.401    0.0454  1    \n3 cor__(Intercept).SpeechStyle      NA       NA      NA    \n4 sd__(Intercept)                    0.0677  NA      NA    \n5 sd__Observation                    0.299   NA      NA    \n6 sd__SpeechStyle                    0.102   NA      NA    \n\n\n\n\n\n\n\nNow that we have this pipeline set up, it becomes easy to adapt the code to try out different parameter values. Let’s explore the effect of repeated measures on power. Try to run a power analysis with each subject receiving two items in each speech style. What happens to the estimate of statistical power?\n\n\nShow the code\n# read saved simulation data\nsims &lt;- read_csv(\"EFsimulations2Stimuli.csv\", show_col_types = FALSE) %&gt;%\n    dplyr::select(term, estimate, std.error, p.value)\n\nsims %&gt;%\n    group_by(term) %&gt;%\n    dplyr::summarize(mean_estimate = mean(estimate), mean_se = mean(std.error),\n        power = mean(p.value &lt; 0.05), .groups = \"drop\")\n\n\n# A tibble: 6 × 4\n  term                         mean_estimate mean_se  power\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)                        0.00141  0.0622  0.046\n2 SpeechStyle                        0.398    0.0857  0.782\n3 cor__(Intercept).SpeechStyle      NA       NA      NA    \n4 sd__(Intercept)                    0.0735  NA      NA    \n5 sd__Observation                    0.291   NA      NA    \n6 sd__SpeechStyle                    0.136   NA      NA    \n\n\n\n\n\nLet’s imagine a scenario where we are interested in the effect of age on IDS preference. We would like to explore the extent to which we can detect a cross-sectional age effect given only two stimulus items per participant. How would adapt the above code to explore this experimental design?\n\n\nShow the code\n# set up the custom data simulation function\nSimulateEFDataWithAge &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  beta_0 = 0,   # grand mean\n  beta_1 =  0.1,   # effect of category\n  beta_as = 0.4,\n  S_as = 0.2,\n  item_sd =  0.2,   # by-item random intercept sd\n  tau_0 = 0.1,   # by-subject random intercept sd\n  tau_1 =  0.1,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 0.3) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_sd)) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = 0, \"IDS\" = 1))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1, S_as), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\", \"S_as\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\")) %&gt;%\n  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())\n  \n  ParameterValues %&gt;%\n    mutate(EF = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + e_si) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, EF)\n}\n\nEFDataWithAgeSimulated &lt;- SimulateEFDataWithAge()\n\n\n\n\nShow the code\nEFDataWithAgeSimulated &lt;- SimulateEFDataWithAge()\nEFDataWithAgeSimulated %&gt;%\n    ggplot() + geom_point(aes(y = EF, x = age_subj, color = subj_id),\n    alpha = 0.6, size = 1, show.legend = F) + geom_smooth(method = \"lm\",\n    se = TRUE, formula = y ~ x, aes(y = EF, x = age_subj)) +\n    ggtitle(\"Age as Positive Interaction Effect\") + xlab(\"Age (standardised age)\") +\n    facet_wrap(~Register) + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# read saved simulation data\nsims &lt;- read_csv(\"EFsimulationsAge.csv\", show_col_types = FALSE) %&gt;%\n    dplyr::select(term, estimate, std.error, p.value)\n\nsims %&gt;%\n    group_by(term) %&gt;%\n    dplyr::summarize(mean_estimate = mean(estimate), mean_se = mean(std.error),\n        power = mean(p.value &lt; 0.05), .groups = \"drop\")\n\n\n# A tibble: 6 × 4\n  term                         mean_estimate mean_se  power\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)                         0.0516  0.0786  0.09 \n2 SpeechStyle:age_subj                0.396   0.153   0.702\n3 cor__(Intercept).SpeechStyle       NA      NA      NA    \n4 sd__(Intercept)                     0.141  NA      NA    \n5 sd__Observation                     0.300  NA      NA    \n6 sd__SpeechStyle                     0.127  NA      NA    \n\n\nNow we have a pretty great pipeline set up that allows us to explore the effects of different parameter values on our ability to detect effects. However, instead of manually varying the parameters one by one, it would be nice if we could set up a grid search to explore values and put the power results into perspective. We will explore how to do this in the next exercise sheet. The code gets slightly more complepx, so make sure that you have understood the code that we have written so far before venturing further.",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html#simulating-effect-size-data-and-making-informed-choices",
    "href": "content/ModellingTheData.html#simulating-effect-size-data-and-making-informed-choices",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "Let’s continue with our IDS preference example and take inspiration from the ManyBabies1 estimate (https://doi.org/10.1177/2515245919900809) to think about how we would simulate data for a new experimental study on the IDS preference effect. Because we are still interested in a within-subjects, between-items study, we can rely on the simulation function from the previous page. All we have to do is to adapt the simulation function so that it suits the new scale of effect sizes. Effect size is a simple way of quantifying the size of the difference between two groups. It allows us to move beyond the “Does it work?” question to “How well does it work in a range of contexts?”. In short, the effect size is based on the standardised mean difference, and quantifies the difference between two means in terms of standard deviation units; an effect size of 0.35 thus implies a difference between the two groups of 0.35 standard deviations.",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html#adapting-our-simulation-function",
    "href": "content/ModellingTheData.html#adapting-our-simulation-function",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "Let’s adapt our simulation function from previous pages to the new scale of effect sizes and call it SimulateEffectSizeData(). Following ManyBabies Consortium (2020), a positive effect size denotes longer looking times to IDS stimuli over ADS stimuli, and an effect size of 0 denotes no preference for either speech style (i.e., similar looking times to ADS and IDS stimuli).\n\n# set up the custom data simulation function\nSimulateEffectSizeData &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  mean_intercept = 0,   # grand mean\n  mean_slope =  0.35,   # effect of category\n  item_varyingintercept =  0.05,   # by-item random intercept sd\n  item_varyingslope =  0.05,   # by-item random intercept sd\n  subject_varyingintercept = 0.1,   # by-subject random intercept sd\n  subject_varyingslope =  0.1,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 0.3) { # residual (standard deviation)\n\n    items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  faux::rnorm_multi(\n  n = n_ADS + n_IDS, \n  mu = 0, # means for random effects are always 0\n  sd = c(item_varyingintercept, item_varyingslope),\n  r = rho,\n  varnames = c(\"item_sd\", \"item_slope_sd\"))) %&gt;%\n    mutate(item_id = faux::make_id(n_ADS + n_IDS, \"I\")) %&gt;%\n    mutate(SpeechStyle = recode(Register, \"ADS\" = 0, \"IDS\" = 1))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(subject_varyingintercept, subject_varyingslope), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"subject_sd\", \"subject_slope_sd\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\"))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())\n  \n  ParameterValues %&gt;%\n    mutate(EF = mean_intercept + subject_sd + item_sd + (mean_slope + subject_slope_sd + item_varyingslope) * SpeechStyle + e_si) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, EF)\n}\n\n\nEffectSizeDataSimulated &lt;- SimulateEffectSizeData()\nEffectSizeDataSimulated %&gt;%\n    ggplot() + geom_density(aes(EF, fill = Register), alpha = 0.8) +\n    geom_vline(xintercept = 0.35, linetype = 3) + geom_vline(xintercept = 0,\n    linetype = 3) + xlim(c(-5 * 0.35, 5 * 0.35)) + ggtitle(\"IDS Preference Effect in Effect Sizes\") +\n    plot_theme + scale_fill_brewer(palette = \"Dark2\") + theme(axis.title.y = element_blank(),\n    axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nEffectSizeDataSimulated %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(EF), .groups = \"drop\") %&gt;%\n    ggplot(aes(x = Register, y = medLT, fill = Register)) + geom_rain(alpha = 0.8,\n    rain.side = \"f1x1\", id.long.var = \"subj_id\", point.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42))) + scale_fill_brewer(palette = \"Dark2\") +\n    ggtitle(\"Subject-Level Differences across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Effect Size\") + plot_theme",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html#a-linear-mixed-effects-model-of-simulated-effect-size-data",
    "href": "content/ModellingTheData.html#a-linear-mixed-effects-model-of-simulated-effect-size-data",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "Let’s think about how we want to run a linear mixed-effects model of the data. For a varying-intercepts, varying-slopes model for each subject, we could run the following model with the lmer syntax as follows:\n\nEF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n\nIn this model, EF is the effect size response variable; 1 corresponds to the average intercept; SpeechStyle is the predictor for the ADS/IDS manipulation for item i; (1 | item_id) specifies a by-subject random intercept (O_0i); (1 + SpeechStyle | subj_id) specifies a subject-specific random intercept (T_0s) plus the subject-specific random slope of SpeechStyle (T_1s). The error term (e_si) is automatically included in all models, so is left implicit in the above formula. The terms in parentheses with the “pipe” separator (|) define the random effects structure. For each of these bracketed terms, the left-hand side of the pipe names the effects you wish to allow to vary and the right hand side names the variable identifying the levels of the random factor over which the terms vary (e.g., subjects or items). The first term, (1 | item_id) allows the intercept (1) to vary over the random factor of items (item_id). This is an instruction to estimate the parameter underlying the O_0i values, namely omega_0. The second term, (1 + X_i | subj_id), allows both the intercept and the effect of category (coded by X_i) to vary over the random factor of subjects (subj_id). It is an instruction to estimate the three parameters that underlie the T_0s and T_1s values, namely tau_0, tau_1, and rho.\n\ndataSimulated &lt;- SimulateEffectSizeData()\n\nmodel &lt;- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle |\n    subj_id), data = dataSimulated)\n\nsummary(model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: EF ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id)\n   Data: dataSimulated\n\nREML criterion at convergence: 191.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.92232 -0.63935 -0.01405  0.65895  2.81429 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj_id  (Intercept) 0.011522 0.10734      \n          SpeechStyle 0.006000 0.07746  0.98\n item_id  (Intercept) 0.005294 0.07276      \n Residual             0.082007 0.28637      \nNumber of obs: 384, groups:  subj_id, 24; item_id, 16\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept) -0.02371    0.03961 20.33685  -0.599    0.556    \nSpeechStyle  0.41100    0.04927 14.95746   8.341 5.23e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nSpeechStyle -0.385\n\n\nTry to run this a couple of times with new simulated data and see how the estimates and significance measures change. In the following code blocks, we will automatise this process and run the power analysis proper!",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html#time-to-power-up-the-analysis",
    "href": "content/ModellingTheData.html#time-to-power-up-the-analysis",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "Now we have two essential components to perform a simulation-based power analysis: i) a code pipeline to generate data for our research question and ii) a clear idea of how we want to model the data. Now it’s time to run the actual power analysis. The way we do this is to specify an effect, run 100s of models and count how many of the models show significant effects. To accomplish this, we can write a new function that combines a modelling component into our previous SimulateEffectSizeData() function. That is, if we include both the simulation and modelling of the data in one function, we can simplify the process of performing a power analysis. We will call this function SimulateAndModelEFData(), and we will use broom.mixed::tidy(model) to obtain a dataframe with relevant results from the model and write them to a .csv-file.\n\n# simulate, analyze, and return a table of parameter\n# estimates\nSimulateAndModelEFData &lt;- function(...) {\n    # simulate EF data function\n    dataSimulated &lt;- SimulateEffectSizeData()\n\n    # model EF data\n    model &lt;- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 +\n        SpeechStyle | subj_id), data = dataSimulated)\n    # write to a dataframee\n    broom.mixed::tidy(model)\n}",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html#running-the-power-analysis",
    "href": "content/ModellingTheData.html#running-the-power-analysis",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "Now we have a function to generate data, run our model and spit out the results! Now it’s time to repeat a few hundred times, so that we can calculate how much power we have with our given parameters. We are going to use map_df() to run the simulation and modelling function 500 times and write it to a .csv file.\nIf it ran correctly, it should have produced a .csv file with model results from each new simulation of data. Let’s read in the results and have a look at what they say!\n\n# read saved simulation data\nsims &lt;- read_csv(\"EFsimulations.csv\", show_col_types = FALSE) %&gt;%\n    dplyr::select(term, estimate, std.error, p.value)\n\nsims %&gt;%\n    group_by(term) %&gt;%\n    dplyr::summarize(mean_estimate = mean(estimate), mean_se = mean(std.error),\n        power = mean(p.value &lt; 0.05), .groups = \"drop\")\n\n# A tibble: 6 × 4\n  term                         mean_estimate mean_se  power\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)                        0.00207  0.0343  0.036\n2 SpeechStyle                        0.401    0.0454  1    \n3 cor__(Intercept).SpeechStyle      NA       NA      NA    \n4 sd__(Intercept)                    0.0677  NA      NA    \n5 sd__Observation                    0.299   NA      NA    \n6 sd__SpeechStyle                    0.102   NA      NA",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/ModellingTheData.html#exercises-to-check-understanding",
    "href": "content/ModellingTheData.html#exercises-to-check-understanding",
    "title": "Part II, Modelling the Simulated Data",
    "section": "",
    "text": "Now that we have this pipeline set up, it becomes easy to adapt the code to try out different parameter values. Let’s explore the effect of repeated measures on power. Try to run a power analysis with each subject receiving two items in each speech style. What happens to the estimate of statistical power?\n\n\nShow the code\n# read saved simulation data\nsims &lt;- read_csv(\"EFsimulations2Stimuli.csv\", show_col_types = FALSE) %&gt;%\n    dplyr::select(term, estimate, std.error, p.value)\n\nsims %&gt;%\n    group_by(term) %&gt;%\n    dplyr::summarize(mean_estimate = mean(estimate), mean_se = mean(std.error),\n        power = mean(p.value &lt; 0.05), .groups = \"drop\")\n\n\n# A tibble: 6 × 4\n  term                         mean_estimate mean_se  power\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)                        0.00141  0.0622  0.046\n2 SpeechStyle                        0.398    0.0857  0.782\n3 cor__(Intercept).SpeechStyle      NA       NA      NA    \n4 sd__(Intercept)                    0.0735  NA      NA    \n5 sd__Observation                    0.291   NA      NA    \n6 sd__SpeechStyle                    0.136   NA      NA    \n\n\n\n\n\nLet’s imagine a scenario where we are interested in the effect of age on IDS preference. We would like to explore the extent to which we can detect a cross-sectional age effect given only two stimulus items per participant. How would adapt the above code to explore this experimental design?\n\n\nShow the code\n# set up the custom data simulation function\nSimulateEFDataWithAge &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  beta_0 = 0,   # grand mean\n  beta_1 =  0.1,   # effect of category\n  beta_as = 0.4,\n  S_as = 0.2,\n  item_sd =  0.2,   # by-item random intercept sd\n  tau_0 = 0.1,   # by-subject random intercept sd\n  tau_1 =  0.1,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 0.3) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_sd)) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = 0, \"IDS\" = 1))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1, S_as), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\", \"S_as\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\")) %&gt;%\n  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())\n  \n  ParameterValues %&gt;%\n    mutate(EF = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + e_si) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, EF)\n}\n\nEFDataWithAgeSimulated &lt;- SimulateEFDataWithAge()\n\n\n\n\nShow the code\nEFDataWithAgeSimulated &lt;- SimulateEFDataWithAge()\nEFDataWithAgeSimulated %&gt;%\n    ggplot() + geom_point(aes(y = EF, x = age_subj, color = subj_id),\n    alpha = 0.6, size = 1, show.legend = F) + geom_smooth(method = \"lm\",\n    se = TRUE, formula = y ~ x, aes(y = EF, x = age_subj)) +\n    ggtitle(\"Age as Positive Interaction Effect\") + xlab(\"Age (standardised age)\") +\n    facet_wrap(~Register) + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# read saved simulation data\nsims &lt;- read_csv(\"EFsimulationsAge.csv\", show_col_types = FALSE) %&gt;%\n    dplyr::select(term, estimate, std.error, p.value)\n\nsims %&gt;%\n    group_by(term) %&gt;%\n    dplyr::summarize(mean_estimate = mean(estimate), mean_se = mean(std.error),\n        power = mean(p.value &lt; 0.05), .groups = \"drop\")\n\n\n# A tibble: 6 × 4\n  term                         mean_estimate mean_se  power\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)                         0.0516  0.0786  0.09 \n2 SpeechStyle:age_subj                0.396   0.153   0.702\n3 cor__(Intercept).SpeechStyle       NA      NA      NA    \n4 sd__(Intercept)                     0.141  NA      NA    \n5 sd__Observation                     0.300  NA      NA    \n6 sd__SpeechStyle                     0.127  NA      NA    \n\n\nNow we have a pretty great pipeline set up that allows us to explore the effects of different parameter values on our ability to detect effects. However, instead of manually varying the parameters one by one, it would be nice if we could set up a grid search to explore values and put the power results into perspective. We will explore how to do this in the next exercise sheet. The code gets slightly more complepx, so make sure that you have understood the code that we have written so far before venturing further.",
    "crumbs": [
      "Part II, ModellingTheData"
    ]
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "1 Turn on GitHub Pages for your repo",
    "text": "1 Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "2 Do your first publish to gh-pages",
    "text": "2 Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "3 Don’t like using gh-pages?",
    "text": "3 Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/ExerciseDataSimulation.html",
    "href": "content/ExerciseDataSimulation.html",
    "title": "Part I, Data Simulation",
    "section": "",
    "text": "In this first task, we will deal with a domain that should be familiar to many developmental science researchers: infant looking times to adult-directed speech (ADS) and infant-directed speech (IDS). In this study, infant participants are exposed to recordings of ADS and IDS, and infants’ looking times to an unrelated visual stimulus is recorded as the primary dependent variable. The key question is whether there are any behavioural differences according to the set of stimuli (i.e., ADS vs. IDS) within each participant. In this section, we will simulate data on the scale of looking times (i.e., 0-20,000ms) to gain fammiliarity with the simulation process. In later sections, we will extend the process to simulate data on the scale of effect sizes.\nTo give an overview of this first simulation task, we will simulate data with crossed random factors of subjects and stimuli and visualise the simulated data along the way to gain familiarity with the process. We will end up with a simulation function that can help us when performing the power analysis in Part II.",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#a-description-of-the-example-and-aim-of-the-simulation",
    "href": "content/ExerciseDataSimulation.html#a-description-of-the-example-and-aim-of-the-simulation",
    "title": "Part I, Data Simulation",
    "section": "",
    "text": "In this first task, we will deal with a domain that should be familiar to many developmental science researchers: infant looking times to adult-directed speech (ADS) and infant-directed speech (IDS). In this study, infant participants are exposed to recordings of ADS and IDS, and infants’ looking times to an unrelated visual stimulus is recorded as the primary dependent variable. The key question is whether there are any behavioural differences according to the set of stimuli (i.e., ADS vs. IDS) within each participant. In this section, we will simulate data on the scale of looking times (i.e., 0-20,000ms) to gain fammiliarity with the simulation process. In later sections, we will extend the process to simulate data on the scale of effect sizes.\nTo give an overview of this first simulation task, we will simulate data with crossed random factors of subjects and stimuli and visualise the simulated data along the way to gain familiarity with the process. We will end up with a simulation function that can help us when performing the power analysis in Part II.",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#determining-the-experimental-parameters",
    "href": "content/ExerciseDataSimulation.html#determining-the-experimental-parameters",
    "title": "Part I, Data Simulation",
    "section": "2 Determining the Experimental Parameters",
    "text": "2 Determining the Experimental Parameters\nBefore we can start to simulate data, we need to be very clear about the study design. This clarity is important because we need to define the parameters that govern the process we assume can give rise to the data. If we are building on ManyBabies1, then we are dealing with a within-subjects, between-items study; that is, each and every subject receives both ADS and IDS stimuli (within-subject), but each stimulus is either ADS or IDS (between-items).\nBecause infants are not the most patient of participants, perhaps a realistic study design would allow researchers to expose infants to 8 recordings of ADS and 8 recordings of IDS. And let’s say that a realistic sample size our laboratory would be around 25 participants. This implies a total of 400 obsevations in this study (i.e., 8 + 8 recording stimuli for each of the 25 children). Let’s set these experimental parameters now already\n\n# set number of subjects and items\nn_subj     &lt;- 25 # number of subjects\nn_ADS  &lt;-  8 # number of ADS stimuli\nn_IDS &lt;-  8 # number of IDS stimuli",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#data-generating-parameters",
    "href": "content/ExerciseDataSimulation.html#data-generating-parameters",
    "title": "Part I, Data Simulation",
    "section": "3 Data Generating Parameters",
    "text": "3 Data Generating Parameters\nNow that we have an overview of the experimental design, we can start to consider a reasonable statistical model of the data. In the following sections, we will build up the parameters for a mixed-effects statistical model of the following type, as described in the lecture:\n\\[\nLooking Time = \\beta_0 + \\beta_1 \\cdot \\text{SpeechStyle} + u_{0j}^{(item)} + u_{0i}^{(subj)} + (u_{1i}^{(subj)} \\cdot \\text{SpeechStyle}) + \\varepsilon\n\\]\nAccording to this formula, the looking time for subject, s, on item, i, is decomposed into linear combination of fixed effects (i.e., the population grand mean, β₀, and the effect of SpeechStyle, β₁) and a variety of random effects, such as a by-subject random intercept, u₀ᵢ^{(subj)}, a by-item random intercept u₀ⱼ^{(item)}, a by-subject random slope, (u₁ᵢ^{(subj)} * SpeechStyle), and a trial-level residual error, ε.\nThis formula paves a clear way forward for our data simulation. In the next section, we will build up a statistical model step by step, defining variables in the code as we go along that reflect our choices for parameters.",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#choosing-values-for-fixed-effect-parameters",
    "href": "content/ExerciseDataSimulation.html#choosing-values-for-fixed-effect-parameters",
    "title": "Part I, Data Simulation",
    "section": "4 Choosing Values for Fixed Effect Parameters",
    "text": "4 Choosing Values for Fixed Effect Parameters\nLet’s start by setting the fixed-effect parameter of SpeechStyle (β₀ + β₁*SpeechStyle). How should we set these parameters? First of all, we can be guided by what we know about looking time distributions in infant experiments. For example, we could imagine average infant looking times to be around 7 seconds (i.e., 7000ms), and from ManyBabies1, we know that infants exhibit around X seconds (i.e., 2000ms) longer looking times to IDS stimuli than to ADS stimuli. Let’s go ahead and code these values explicitly as parameter values in our simulation.\n\n# set fixed effect parameters\nmean_intercept &lt;- 7000  # intercept; i.e., the grand mean, β₀\nmean_slope &lt;- 2000  # slope; i.e, effect of IDS, β₁\n\nWhen we’re modelling data from experiments involving individuals (like infants in this case), it’s essential to account for the fact that each individual may have a unique baseline reaction to the stimuli, which we can’t anticipate or control for entirely. Similarly, the effect of the stimuli itself might vary across different instances of the stimulus. In other words, we expect variability both in how subjects react and in the impact of each stimulus. To address this, we introduce random intercept values for subjects and items. This means we allow for each subject and each stimulus item to have its own intercept value, capturing their individual baseline reactions and effects. To quantify the variability around these values, we introduce the standard deviation of the random intercepts, reflecting the range of differences we might observe among subjects’ reactions and the effects of different stimulus items. Let’s code this as standard deviation of by-subject random intercept and by-item random intercept sd.",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#choosing-values-for-varying-slope-parameters",
    "href": "content/ExerciseDataSimulation.html#choosing-values-for-varying-slope-parameters",
    "title": "Part I, Data Simulation",
    "section": "5 Choosing Values for Varying Slope Parameters",
    "text": "5 Choosing Values for Varying Slope Parameters\n\n# set random effect parameters\nsubject_varyingintercept &lt;- 2000  # by-subject random intercept sd\nitem_varyingintercept &lt;- 1000  # by-item random intercept sd\n\nIn modeling the effect of IDS stimuli on infant looking times, it’s also important to acknowledge that this effect may vary across different infants. Some infants might be more responsive to IDS stimuli than others, leading to differences in their looking times. Therefore, we need to introduce subject-specific variability to account for how the effect of IDS varies among infants. In statistical modeling, when we include random slopes, we’re essentially allowing the effect of certain variables to vary not only across different individuals (random intercepts) but also across different levels of another variable (random slopes).\nWhen we introduce random slopes, we need to consider potential correlations between these varying slopes and the varying intercepts. This is because if there’s a correlation between the way individuals (infants, in this case) respond to the IDS stimuli (reflected in the random slopes) and their baseline behaviors (reflected in the random intercepts), it can affect our model’s predictions. For instance, if infants who naturally have longer attention spans (reflected in higher random intercepts) also tend to show stronger responses to IDS stimuli (reflected in steeper random slopes), ignoring this correlation might lead to biased estimates or inaccurate predictions. Including a correlation matrix allows us to explicitly account for these potential correlations, ensuring that our model accurately captures the relationships between different sources of variability in the data and produces more reliable results. To capture these patterns, we include a correlation matrix, specifying a weak correlation between the varying intercepts and varying slopes of infant participants. Lastly, we incorporate a residual error term to account for any unexplained sources of variability in the model. This helps ensure that our model accurately captures the complexities of infants’ responses to different stimuli while also acknowledging inherent variations and correlations within the data.\n\n# set more random effect and error parameters\nsubject_varyingslope &lt;- 1000  # by-subject random slope sd\nrho &lt;- 0.2  # correlation between intercept and slope\nsigma &lt;- 500  # residual (error) sd",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-stimulus-items",
    "href": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-stimulus-items",
    "title": "Part I, Data Simulation",
    "section": "6 Simulate the sampling of stimulus items",
    "text": "6 Simulate the sampling of stimulus items\nNow it’s time to create a dataset that lists, for each stimulus item, the speech style it is in and its varying properties on infants’ looking times. This captures the intuition that we would expect different speech recordings to exhibit variation in infants’ average looking times. Here, we model items only with varying intercepts. To set the parameter for varying item intercept, we are going to specify the standard deviation that we expect items to exhibit in the parameter, item_varyingintercept, in the below code. That is, we sample values from a normal distribution using the rnorm() function, with a meaan of 0 and standard deviation of item_varyingintercept. For the varying item variable, we also need to assign a unique identifer to each of the 16 speech stimuli and designate whether the stimuli are ADS or IDS, with the first 8 being ADS and the next 8 being IDS. We are going to use the faux package to carry this out. Lastly, we will carry out contrast coding, and we will later multiply this effect-coded factor by the fixed effect of category to simulate data where the ADS stimuli on average generate looking times of -500 ms different from the grand mean, while the IDS stimuli are on average 1000 ms different from the grand mean.\n\n# simulate a sample of items\n# total number of items = n_ADS + n_IDS\nitems &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_varyingintercept)\n) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = -0.5, \"IDS\" = +0.5)) #introduce a numeric predictor to represent what category each stimulus item, we set ADS to be -0.5 and IDS to be +0.5. This is what is known as contrast coding.\n\nTo get to better grips with the simuation process, let’s the data visualise and take a look at what we have produced:\n\nglimpse(items)\n\nRows: 16\nColumns: 4\n$ Register    &lt;chr&gt; \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"IDS\", \"A…\n$ O_0i        &lt;dbl&gt; -1207.06575, 277.42924, 1084.44118, -2345.69770, 429.12469…\n$ item_id     &lt;chr&gt; \"I01\", \"I02\", \"I03\", \"I04\", \"I05\", \"I06\", \"I07\", \"I08\", \"I…\n$ SpeechStyle &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -0.5, -0.5, -0.5, …\n\nggplot(items, aes(1, O_0i, fill = Register, color = Register)) +\n    geom_rain(alpha = 0.8, boxplot.args = list(color = \"black\",\n        outlier.shape = NA)) + ggtitle(\"Varying Intercept Terms for Stimulus Item\") +\n    ylab(\"SD of Item Intercept (ms)\") + facet_wrap(~Register) +\n    scale_fill_brewer(palette = \"Dark2\") + scale_color_brewer(palette = \"Dark2\") +\n    plot_theme + theme(axis.title.x = element_blank(), axis.text.x = element_blank(),\n    axis.ticks.x = element_blank())",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-subjects",
    "href": "content/ExerciseDataSimulation.html#simulate-the-sampling-of-subjects",
    "title": "Part I, Data Simulation",
    "section": "7 Simulate the sampling of subjects",
    "text": "7 Simulate the sampling of subjects\nNow, we will simulate the sampling of individual subjects to create a table that lists each subject along with their two correlated varying effects, namely intercepts and slopes. This process is slightly more complex than before because we cannot simply sample the intercept values (𝑇0𝑠) independently from the slope values (𝑇1𝑠) using rnorm(). Instead, we need to sample pairs of ⟨𝑇0𝑠,𝑇1𝑠⟩ values for each subject from a bivariate normal distribution. We will use the rnorm_multi() function from the faux package (DeBruine 2020) to carry this out. This function generates a table of simulated values from a multivariate normal distribution, allowing us to specify the means (mu) and standard deviations (sd) for each variable, along with the correlations (r), which can be a single value applied to all pairs, a correlation matrix, or a vector of values representing the upper right triangle of the correlation matrix.\n\n# simulate a sample of subjects\n# sample from a multivariate random distribution \nsubjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(subject_varyingintercept, subject_varyingslope), # note that we set the SDs further up in the code when specifying varying intercepts and sloeps.\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\")) # add subject ids that correspond to the number of rows simulated.\n\nAgain, let’s visualise this process, so that we are sure what is going on:\n\nglimpse(subjects)\n\nRows: 25\nColumns: 3\n$ T_0s    &lt;dbl&gt; 884.80961, 1710.55508, 1635.49818, -4947.76450, -390.13885, 83…\n$ T_1s    &lt;dbl&gt; 1165.1005, 1063.9211, 489.8264, 327.8312, 899.7244, 1197.1137,…\n$ subj_id &lt;chr&gt; \"S01\", \"S02\", \"S03\", \"S04\", \"S05\", \"S06\", \"S07\", \"S08\", \"S09\",…\n\nsubjects %&gt;%\n    pivot_longer(cols = starts_with(\"T\"), names_to = \"T_variable\",\n        values_to = \"value\") %&gt;%\n    ggplot() + geom_density(aes(value, fill = T_variable), alpha = 0.8) +\n    xlim(c(-4 * subject_varyingintercept, 4 * subject_varyingintercept)) +\n    facet_wrap(~T_variable) + ggtitle(\"Varying Intercept and Slope Terms for Subjects\") +\n    plot_theme + scale_fill_brewer(palette = \"Dark2\") + theme(axis.title.y = element_blank(),\n    axis.text.y = element_blank(), axis.ticks.y = element_blank())",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#time-to-put-it-all-together",
    "href": "content/ExerciseDataSimulation.html#time-to-put-it-all-together",
    "title": "Part I, Data Simulation",
    "section": "8 Time to Put It All Together",
    "text": "8 Time to Put It All Together\nBecause all subjects respond to all items, we can set up a table of trials by making a table with every possible combination of the rows in the subject and item tables using the tidyverse function crossing(). This function generates every possible combination of rows from the subject and item tables, ensuring that each subject’s responses to each item are included in the dataset. To introduce variability reflecting the inherent fluctuations in trial-by-trial performance, we incorporate random error into each trial. This randomness is simulated by sampling values from a normal distribution with a mean of 0 and a standard deviation of sigma. This approach ensures that our dataset captures the full range of subject-item interactions while accounting for unpredictable variations in individual performance across trials.\n\n# cross subject and item IDs; add an error term nrow(.) is\n# the number of rows in the table\nParameterValues &lt;- crossing(subjects, items) %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma))\n\nglimpse(ParameterValues)\n\nRows: 400\nColumns: 8\n$ T_0s        &lt;dbl&gt; -4947.764, -4947.764, -4947.764, -4947.764, -4947.764, -49…\n$ T_1s        &lt;dbl&gt; 327.8312, 327.8312, 327.8312, 327.8312, 327.8312, 327.8312…\n$ subj_id     &lt;chr&gt; \"S04\", \"S04\", \"S04\", \"S04\", \"S04\", \"S04\", \"S04\", \"S04\", \"S…\n$ Register    &lt;chr&gt; \"ADS\", \"ADS\", \"ADS\", \"ADS\", \"ADS\", \"ADS\", \"ADS\", \"ADS\", \"I…\n$ O_0i        &lt;dbl&gt; -998.38644, -890.03783, -776.25389, -564.45200, -477.19270…\n$ item_id     &lt;chr&gt; \"I12\", \"I10\", \"I13\", \"I09\", \"I11\", \"I16\", \"I14\", \"I15\", \"I…\n$ SpeechStyle &lt;dbl&gt; -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 0.5, 0.5, …\n$ e_si        &lt;dbl&gt; -569.303868, 683.913590, 664.782395, 168.236399, 3.446419,…\n\n\nNow we have specified the parameters in ParameterValues, we are ready to add up everything together to create the response variable (i.e., infant looking times in milliseconds). To be more specific, we calculate the response variable, looking time, by adding together:\n\nthe grand intercept (mean_intercept),\neach subject-specific random intercept (T_0s),\neach item-specific random intercept (O_0i),\neach sum of the speech style effect (mean_slope) and the random slope (T_1s), multiplied by the numeric predictor (SpeechStyle), and\neach residual error (e_si).\n\n\n# calculate the response variable\nSimulatedLT &lt;- ParameterValues %&gt;%\n  mutate(LT = mean_intercept + T_0s + O_0i + (mean_slope + T_1s) * SpeechStyle + e_si) %&gt;% #sum together overall intercept, varying subject and item intercepts, varying subject slopes, and random error.\n  mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %&gt;% #add a long tail to the distribution to simulate exgaussian distribution of looking times\n  dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)\n\nLet’s have a look at what the data we have generated looks like:\n\n# Plot of how overall looking time distributions differ\n# across ADS and IDS\nSimulatedLT %&gt;%\n    ggplot() + geom_density(aes(LT, fill = Register), alpha = 0.8) +\n    ggtitle(\"Varying Intercept and Slope Terms for Subjects\") +\n    plot_theme + scale_fill_brewer(palette = \"Dark2\") + theme(axis.title.y = element_blank(),\n    axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n# Plot of how looking times of indvidual subjects differ\n# across the two speech style\nSimulatedLT %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(LT), .groups = \"drop\") %&gt;%\n    ggplot(aes(x = Register, y = medLT, fill = Register)) + geom_rain(alpha = 0.8,\n    rain.side = \"f1x1\", id.long.var = \"subj_id\", point.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42))) + scale_fill_brewer(palette = \"Dark2\") +\n    ggtitle(\"Individual Subject Looking Times across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Looking Time (ms)\") + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#with-the-initial-setup-done-lets-automatise",
    "href": "content/ExerciseDataSimulation.html#with-the-initial-setup-done-lets-automatise",
    "title": "Part I, Data Simulation",
    "section": "9 With the Initial Setup Done, Let’s Automatise!",
    "text": "9 With the Initial Setup Done, Let’s Automatise!\nNow that we’ve simulated a dataset with the necessary properties, suitable for sophisticated linear mixed effects models, we can streamline the process by encapsulating all the preceding code into a custom function. This function will accept the parameters we defined earlier as arguments, with default values set to our chosen parameters. By doing so, we are empowered to effortlessly experiment with different parameters or generate multiple datasets for power analysis purposes. The following code condenses all the preceding steps into a single function, returning a dataset with the specified parameters.\n\n# set up the custom data simulation function\nSimulateLTData &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  mean_intercept = 7000,   # grand mean\n  mean_slope =  2000,   # effect of category\n  item_varyingintercept =  200,   # by-item random intercept sd\n  subject_varyingintercept = 2000,   # by-subject random intercept sd\n  subject_varyingslope =  1000,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 500) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_varyingintercept)) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = -0.5, \"IDS\" = +0.5))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(subject_varyingintercept, subject_varyingslope), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\"))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())\n  \n  ParameterValues %&gt;%\n    mutate(LT = mean_intercept + T_0s + O_0i + (mean_slope + T_1s) * SpeechStyle + e_si) %&gt;%\n    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)\n}",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/ExerciseDataSimulation.html#exercises-to-check-understanding",
    "href": "content/ExerciseDataSimulation.html#exercises-to-check-understanding",
    "title": "Part I, Data Simulation",
    "section": "10 Exercises to Check Understanding",
    "text": "10 Exercises to Check Understanding\n\n10.1 Exercise I\n\nHow would you adapt the above code to generate a dataset with 500 participants and no effect of SpeechStyle (i.e., distributions similar to the below plot)?\n\n\n\nShow the code\n# With our new SimulateLTData() function, the answer here\n# is fairly straightforward! We can simply specify that we\n# want to simulate 500 subjects and want a mean slope of 0,\n# like so: SimulateLTData(n_subj = 500, mean_slope = 0).\n\nLTDataSimulated &lt;- SimulateLTData(n_subj = 500, mean_slope = 0)\n\nLTDataSimulated %&gt;%\n    group_by(subj_id, Register) %&gt;%\n    dplyr::summarise(medLT = mean(LT), .groups = \"drop\") %&gt;%\n    ggplot(aes(x = Register, y = medLT, fill = Register)) + geom_rain(alpha = 0.8,\n    rain.side = \"f1x1\", id.long.var = \"subj_id\", point.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04,\n        height = 0, seed = 42))) + scale_fill_brewer(palette = \"Dark2\") +\n    ggtitle(\"Looking Time Differences across Speech Styles\") +\n    xlab(\"Speech Style\") + ylab(\"Looking Time (ms)\") + scale_color_manual(values = viridis(n = 27)) +\n    plot_theme\n\n\n\n\n\n\n\n\n\n\n\n10.2 Exercise II\n\nWe might expect the IDS preference effect to change with infant age, such that older infants display longer looking times to IDS over ADS. How would you add a positive interaction effect of (cross-sectional) age as a predictor to the model (hint: it involves randomly sampling age for each child and adding an effect to the simulation code and model)? Try to think through the problem, get inspiration from the below plot, code up a solution, and only then click on “Show the code” to check how you might approach this.\n\n\n\nShow the code\n#The question here involves adding infant age as an interaction effect with SpeechStyle. We will approach this question by modifying the code that simulates subject-level data. Here, we will sample an age variable and pretend that we have standardised age so that its values fall between -0.5 and 0.5. We thus randomly sample and age to assign one age to each subject. We also need to specify a slope value for the influence of subject ag and place it in the start of the function; we will add subject_age = 200. Lastly, we need to change how we sum the values together, so that age has an effect on looking times, but also that the influence of age exerts different effects across the two speech styles.\n\n# set up the custom data simulation function\nSimulateLTDataWithAge &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  beta_0 = 7000,   # grand mean\n  beta_1 =  2000,   # effect of category\n  beta_as = 5000,\n  S_as = 200,\n  item_sd =  200,   # by-item random intercept sd\n  tau_0 = 2000,   # by-subject random intercept sd\n  tau_1 =  1000,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 500) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = item_sd)) %&gt;% \n  mutate(item_id = faux::make_id(nrow(.), \"I\")) %&gt;%\n  mutate(SpeechStyle = recode(Register, \"ADS\" = 0, \"IDS\" = 1))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1, S_as), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\", \"S_as\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\")) %&gt;%\n  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())\n  \n  ParameterValues %&gt;%\n    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + e_si) %&gt;%\n    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, LT)\n}\n\nDataWithAgeSimulated &lt;- SimulateLTDataWithAge()\nDataWithAgeSimulated %&gt;%\nggplot() + \n  geom_point(aes(y = LT, x = age_subj, color = subj_id), alpha = 0.6, size = 1, show.legend = F) + \n  geom_smooth(method = \"lm\", se = TRUE, formula = y ~ x, aes(y = LT, x = age_subj)) +\n  ggtitle(\"Age as Positive Interaction Effect\") +\n  xlab(\"Age (standardised age)\") + \n  facet_wrap(~Register) +\n  scale_color_manual(values = viridis(n = 27)) +\n  plot_theme\n\n\n\n\n\n\n\n\n\n\n\n10.3 Exercise III\n\nAs mentioned above, we made the simplifying assumption that each and every stimulus item evokes exactly the same response in participants; however, certain items might elicit stronger or weaker responses depending on individual differences or contextual factors, so including varying slopes for stimulus items can help capture this heterogeneity more accurately. How could we modify the above code to include varying slopes according stimulus items? Again, try to think through the problem, get inspiration from the below plot, code up a solution, and only then click on “Show the code” to check how you might approach this.\n\n\n\nShow the code\n# set up the custom data simulation function\nSimulateLTDataWithAge &lt;- function(\n  n_subj = 24,   # number of subjects\n  n_ADS  = 8,   # number of ingroup stimuli\n  n_IDS =  8,   # number of outgroup stimuli\n  beta_0 = 7000,   # grand mean\n  beta_1 =  2000,   # effect of category\n  beta_as = 5000,\n  S_as = 200,\n  item_sd =  200,   # by-item random intercept sd\n  item_slope_sd =  200,   # by-item random slope sd\n  tau_0 = 2000,   # by-subject random intercept sd\n  tau_1 =  1000,   # by-subject random slope sd\n  rho = 0.2,   # correlation between intercept and slope\n  sigma = 500) { # residual (standard deviation)\n\n  items &lt;- data.frame(\n  #item_id = seq_len(n_ADS + n_IDS),\n  Register = rep(c(\"IDS\", \"ADS\"), c(n_ADS, n_IDS)),\n  faux::rnorm_multi(\n  n = n_ADS + n_IDS, \n  mu = 0, # means for random effects are always 0\n  sd = c(item_sd, item_slope_sd),\n  r = rho,\n  varnames = c(\"item_sd\", \"item_slope_sd\"))) %&gt;%\n    mutate(item_id = faux::make_id(n_ADS + n_IDS, \"I\")) %&gt;%\n    mutate(SpeechStyle = recode(Register, \"ADS\" = 0, \"IDS\" = 1))\n\n  # simulate a sample of subjects\n\n# sample from a multivariate random distribution \n  subjects &lt;- faux::rnorm_multi(\n  n = n_subj, \n  mu = 0, # means for random effects are always 0\n  sd = c(tau_0, tau_1, S_as), # set SDs\n  r = rho, # set correlation, see ?faux::rnorm_multi\n  varnames = c(\"T_0s\", \"T_1s\", \"S_as\")\n) %&gt;%\n  mutate(subj_id = faux::make_id(nrow(.), \"S\")) %&gt;%\n  mutate(age_subj = runif(n_subj, min = -0.5, max = 0.5))\n\n  ParameterValues &lt;- crossing(subjects, items)  %&gt;%\n    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, S_as, everything())\n  \n  ParameterValues %&gt;%\n    mutate(LT = beta_0 + T_0s + item_sd + (beta_1 + T_1s) * SpeechStyle + ((beta_as + S_as) * age_subj * SpeechStyle) + (beta_1 + item_slope_sd) * SpeechStyle + e_si) %&gt;%\n    mutate(LT = LT + rexp(nrow(.), rate = 0.0003)) %&gt;%\n    dplyr::select(subj_id, item_id, Register, SpeechStyle, age_subj, LT)\n}\n\nDataWithAgeSimulated &lt;- SimulateLTDataWithAge()\nDataWithAgeSimulated %&gt;%\nggplot() + \n  geom_point(aes(y = LT, x = age_subj, color = subj_id), alpha = 0.6, size = 1, show.legend = F) + \n  geom_smooth(method = \"lm\", se = TRUE, formula = y ~ x, aes(y = LT, x = age_subj)) +\n  ggtitle(\"Age as Positive Interaction Effect\") +\n  xlab(\"Age (standardised age)\") + \n  facet_wrap(~Register) +\n  scale_color_manual(values = viridis(n = 27)) +\n  plot_theme",
    "crumbs": [
      "Part I, Data Simulation"
    ]
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "1 R Markdown",
    "text": "1 R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "2 Including Plots",
    "text": "2 Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/GridSearch.html",
    "href": "content/GridSearch.html",
    "title": "Grid Searches and Sensitivity Analyses",
    "section": "",
    "text": "It should be clear from the previous sections that the data simulation process involves a multiverse of experimenter choices. One way to explore (and calm our fears about) the respective importance of these individual choices and their interactions would be to conduct a sensitivity analysis with a grid search of all available combinations among parameter values of interest. To do this, we can wrap our Simulation and Modelling function with a higher-level function that inputs a series of parameter combinations that we are interested in exploring further. Here is a suggestion for a function of this type. We can start by creating a matrix of parameter combinations that we are interested in.\n\nsubj_n &lt;- seq(2, 50, by = 3)\ntrial_n &lt;- seq(4, 8, by = 2)\nNumberOfModels &lt;- 500\n\nparam_combinations &lt;- expand.grid(subj_n = subj_n, trial_n = trial_n)\n\n\nrun_sims_grid_point &lt;- function(filename_full, trial_n, subj_n) {\n    ADS_n = trial_n/2\n    IDS_n = trial_n/2\n    n_subj = subj_n\n\n    dataSimulated &lt;- SimulateEffectSizeData(n_subj = n_subj,\n        n_ADS = ADS_n, n_IDS = IDS_n)\n\n    model &lt;- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 +\n        SpeechStyle | subj_id), data = dataSimulated)\n\n    sim_results &lt;- broom.mixed::tidy(model)\n\n    # append the results to a file\n    append &lt;- file.exists(filename_full)\n    write_csv(sim_results, filename_full, append = append)\n\n    # return the tidy table\n    sim_results\n}\n\n\nfor (i in seq_len(nrow(param_combinations))) {\n    sim_params &lt;- param_combinations[i, ]\n    filename_full &lt;- paste0(here(\"sims_grid_search/test_grid_search_\"),\n        sim_params$subj_n, \"_\", sim_params$trial_n, \".csv\")\n    start_time &lt;- Sys.time()  # Start time\n    sims &lt;- purrr::map_df(1:NumberOfModels, ~run_sims_grid_point(filename_full = filename_full,\n        subj_n = sim_params$subj_n, trial_n = sim_params$trial_n))\n    end_time &lt;- Sys.time()  # End time\n    cat(\"Simulation\", i, \"Time elapsed:\", end_time - start_time,\n        \"\\n\")\n}\n\n\nsetwd(here(\"sims_grid_search\"))\nfile_names &lt;- list.files(pattern = \"*.csv\")\n\n# read in all CSV files into a list of dataframes\ndf_list &lt;- purrr::map(file_names, ~{\n    df &lt;- read.csv(.x)\n    df$filename &lt;- .x\n    df\n})\n\ndf &lt;- purrr::reduce(df_list, dplyr::bind_rows)\n\ndf_per_sim &lt;- df %&gt;%\n    filter(effect == \"fixed\") %&gt;%\n    filter(term == \"SpeechStyle\") %&gt;%\n    group_by(filename) %&gt;%\n    summarise(median_estimate = median(estimate), median_se = median(std.error),\n        power = mean(p.value &lt; 0.05))\n\nPowerGridData &lt;- df_per_sim %&gt;%\n    mutate(n_subj = as.numeric(sapply(strsplit(filename, \"_\"),\n        `[`, 4)), n_trial = as.factor(str_replace(sapply(strsplit(filename,\n        \"_\"), `[`, 5), pattern = \".csv\", \"\")))\n\nggplot(PowerGridData) + geom_point(aes(x = n_subj, y = power,\n    color = n_trial)) + geom_line(aes(x = n_subj, y = power,\n    color = n_trial)) + geom_hline(yintercept = 0.8, linetype = 3) +\n    xlab(\"Sample Size\") + ylab(\"Statistical Power\") + ggtitle(\"Interaction among Number of Subjects & Repeated Measures\") +\n    scale_color_brewer(palette = \"Dark2\") + plot_theme",
    "crumbs": [
      "Part III, Grid Searches and Sensitivity Analyses"
    ]
  },
  {
    "objectID": "content/GridSearch.html#using-grid-searches-to-explore-the-multiverse",
    "href": "content/GridSearch.html#using-grid-searches-to-explore-the-multiverse",
    "title": "Grid Searches and Sensitivity Analyses",
    "section": "",
    "text": "It should be clear from the previous sections that the data simulation process involves a multiverse of experimenter choices. One way to explore (and calm our fears about) the respective importance of these individual choices and their interactions would be to conduct a sensitivity analysis with a grid search of all available combinations among parameter values of interest. To do this, we can wrap our Simulation and Modelling function with a higher-level function that inputs a series of parameter combinations that we are interested in exploring further. Here is a suggestion for a function of this type. We can start by creating a matrix of parameter combinations that we are interested in.\n\nsubj_n &lt;- seq(2, 50, by = 3)\ntrial_n &lt;- seq(4, 8, by = 2)\nNumberOfModels &lt;- 500\n\nparam_combinations &lt;- expand.grid(subj_n = subj_n, trial_n = trial_n)\n\n\nrun_sims_grid_point &lt;- function(filename_full, trial_n, subj_n) {\n    ADS_n = trial_n/2\n    IDS_n = trial_n/2\n    n_subj = subj_n\n\n    dataSimulated &lt;- SimulateEffectSizeData(n_subj = n_subj,\n        n_ADS = ADS_n, n_IDS = IDS_n)\n\n    model &lt;- lmer(EF ~ 1 + SpeechStyle + (1 | item_id) + (1 +\n        SpeechStyle | subj_id), data = dataSimulated)\n\n    sim_results &lt;- broom.mixed::tidy(model)\n\n    # append the results to a file\n    append &lt;- file.exists(filename_full)\n    write_csv(sim_results, filename_full, append = append)\n\n    # return the tidy table\n    sim_results\n}\n\n\nfor (i in seq_len(nrow(param_combinations))) {\n    sim_params &lt;- param_combinations[i, ]\n    filename_full &lt;- paste0(here(\"sims_grid_search/test_grid_search_\"),\n        sim_params$subj_n, \"_\", sim_params$trial_n, \".csv\")\n    start_time &lt;- Sys.time()  # Start time\n    sims &lt;- purrr::map_df(1:NumberOfModels, ~run_sims_grid_point(filename_full = filename_full,\n        subj_n = sim_params$subj_n, trial_n = sim_params$trial_n))\n    end_time &lt;- Sys.time()  # End time\n    cat(\"Simulation\", i, \"Time elapsed:\", end_time - start_time,\n        \"\\n\")\n}\n\n\nsetwd(here(\"sims_grid_search\"))\nfile_names &lt;- list.files(pattern = \"*.csv\")\n\n# read in all CSV files into a list of dataframes\ndf_list &lt;- purrr::map(file_names, ~{\n    df &lt;- read.csv(.x)\n    df$filename &lt;- .x\n    df\n})\n\ndf &lt;- purrr::reduce(df_list, dplyr::bind_rows)\n\ndf_per_sim &lt;- df %&gt;%\n    filter(effect == \"fixed\") %&gt;%\n    filter(term == \"SpeechStyle\") %&gt;%\n    group_by(filename) %&gt;%\n    summarise(median_estimate = median(estimate), median_se = median(std.error),\n        power = mean(p.value &lt; 0.05))\n\nPowerGridData &lt;- df_per_sim %&gt;%\n    mutate(n_subj = as.numeric(sapply(strsplit(filename, \"_\"),\n        `[`, 4)), n_trial = as.factor(str_replace(sapply(strsplit(filename,\n        \"_\"), `[`, 5), pattern = \".csv\", \"\")))\n\nggplot(PowerGridData) + geom_point(aes(x = n_subj, y = power,\n    color = n_trial)) + geom_line(aes(x = n_subj, y = power,\n    color = n_trial)) + geom_hline(yintercept = 0.8, linetype = 3) +\n    xlab(\"Sample Size\") + ylab(\"Statistical Power\") + ggtitle(\"Interaction among Number of Subjects & Repeated Measures\") +\n    scale_color_brewer(palette = \"Dark2\") + plot_theme",
    "crumbs": [
      "Part III, Grid Searches and Sensitivity Analyses"
    ]
  }
]